{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybl72njA5UVg"
      },
      "source": [
        "# modelDepthMAN\n",
        "The flow described in this notebook implements a complete pipeline for evaluating depth estimation and transformation models on the MAN TruckScenes mini dataset, from data preparation to detailed per-image visual analysis.\n",
        "\n",
        "**Summary of the Flow:**\n",
        "\n",
        "- **Dataset Acquisition and Preparation:**  \n",
        "  The pipeline begins by downloading, extracting, and preparing the MAN TruckScenes mini dataset. Images and corresponding LiDAR point clouds are loaded and preprocessed for both training and testing, with the test set reduced for computational efficiency.\n",
        "\n",
        "- **Relative Depth Estimation:**  \n",
        "  A pre-trained transformer-based model (Depth-Anything V2) is used to generate relative depth heatmaps for every image in the train and test sets.\n",
        "\n",
        "- **Affine Parameter Extraction:**  \n",
        "  For each image, polynomial regression (both linear and quadratic) is used to fit the relationship between relative and absolute depth values, producing the best-fit affine parameters for each sample.\n",
        "\n",
        "- **Model Definition and Training:**  \n",
        "  A custom neural network (ScaleBiasHead) is defined, which takes RGB and relative depth as input and predicts affine transformation parameters. The model is trained using a combination of depth and parameter losses, leveraging the polynomial regression results as supervision.\n",
        "\n",
        "- **Evaluation:**  \n",
        "  The trained model is evaluated on the test set and compared against the polynomial regression baselines, both globally and under specific lighting conditions. Losses are computed for each approach to quantify performance.\n",
        "\n",
        "- **Object Detection and Tangible Metrics:**  \n",
        "  YOLO is used to detect objects and generate bounding boxes in test images. For each detected object, the model’s and polynomial’s depth predictions are compared to LiDAR ground truth, and detailed error metrics are calculated per box, making the evaluation more interpretable and relevant to real-world applications.\n",
        "\n",
        "- **Visualization and Per-Image Analysis:**  \n",
        "  Finally, a single test image is randomly selected for in-depth analysis. The notebook displays the image with detected boxes, overlays depth maps and point cloud data, and visualizes the predicted and baseline depth values within each box. Key metrics and errors are reported, providing a concrete, visual, and quantitative understanding of the model’s performance on a real example.\n",
        "\n",
        "This flow ensures a rigorous, interpretable, and scenario-aware evaluation of depth estimation models, combining classical regression, deep learning, and object detection within a unified framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGKI5B-N5Yu1"
      },
      "source": [
        "## Dataset\n",
        "Downloads and unzips the MAN TruckScenes mini dataset (v1.0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLMoQSzQEv0e",
        "outputId": "97c35789-5242-42d9-d2b4-99fbd2268792"
      },
      "outputs": [],
      "source": [
        "# For v1.0 version of mini dataset\n",
        "!wget https://man-truckscenes.s3.eu-central-1.amazonaws.com/release/mini/man-truckscenes_metadata_v1.0-mini.zip\n",
        "!wget https://man-truckscenes.s3.eu-central-1.amazonaws.com/release/mini/man-truckscenes_sensordata_v1.0-mini.zip\n",
        "!unzip \"man-truckscenes_*.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVX1JzRs5eDq"
      },
      "source": [
        "## Downloads of the Requirement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLIkzd7nn3-R",
        "outputId": "f1441771-258a-4c87-c0ae-d4bf61781916"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install opencv-python matplotlib\n",
        "!pip install truckscenes-devkit[all]\n",
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjol_wtd6eeg"
      },
      "source": [
        "## Imports of the requiremnts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "_h8m0rLdoMAG",
        "outputId": "15a4f900-4b7f-40fa-d61f-795bc41310c8"
      },
      "outputs": [],
      "source": [
        "# Standard library\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "import os.path as osp\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Union\n",
        "\n",
        "# ML & data handling\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gc\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "# Imaging & visualization\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm, rcParams\n",
        "from matplotlib.axes import Axes\n",
        "from matplotlib.colors import Colormap, Normalize\n",
        "from matplotlib.cm import ScalarMappable\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from PIL import Image\n",
        "import open3d as o3d\n",
        "from pyquaternion import Quaternion\n",
        "\n",
        "# TruckScenes devkit\n",
        "from truckscenes import TruckScenes\n",
        "from truckscenes.utils import colormap\n",
        "from truckscenes.utils.data_classes import LidarPointCloud, RadarPointCloud\n",
        "from truckscenes.utils.geometry_utils import view_points, transform_matrix, BoxVisibility\n",
        "\n",
        "# Hugging Face Transformers\n",
        "from transformers import pipeline, AutoImageProcessor, AutoModelForDepthEstimation\n",
        "\n",
        "#Yolo\n",
        "from ultralytics import YOLO\n",
        "\n",
        "from torchvision.transforms import ToTensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR0AAks86lrf"
      },
      "source": [
        "## Utils\n",
        "Declaration of all the functions needed in the scripts\n",
        "- **scaleDepthTo255**: Normalizes depth tensor values to 0-255 range and converts to a NumPy array.  \n",
        "- **scaleDepthTo255n_np**: Performs the same normalization as above but assumes input is already a NumPy array.  \n",
        "- **scaleDepthToIMG**: Converts a normalized depth tensor to an 8-bit PIL Image object.\n",
        "- **extract_box_depth_data**: Processes bounding boxes to calculate quadratic-transformed depth metrics and optional absolute depth averages for each box.\n",
        "- **plot_with_box_depth_rel_abs**: Visualizes an RGB image with bounding boxes, overlays depth statistics, and displays the corresponding transformed depth map with color mapping.\n",
        "- **plot_with_box_depth**: Displays an RGB image with bounding boxes and mean depth annotations, alongside the corresponding depth map.\n",
        "- **plot_with_box_depth_3d**: Similar to the above, but uses a quadratic transformation for depth before visualization.\n",
        "- **map_pointcloud_to_depth**: Projects a point cloud onto a camera image, returning projected points, their colors, the image, and depth values.\n",
        "- **render_pointcloud_in_deapth**: Retrieves and projects a sample's point cloud onto the camera image, returning points, colors, image, and depths.\n",
        "- **plot_with_box_depth2**: Plots an RGB image with bounding boxes and mean (linearly transformed) depth annotations on a single axis.\n",
        "- **plot_with_box_depth3**: Plots an RGB image with bounding boxes and mean (quadratically transformed) depth annotations on a single axis.\n",
        "- **affine_param_loss**: Computes Huber loss between predicted and target affine parameters (a,b,c) with δ-controlled transition.  \n",
        "- **affine_depth_loss**: Calculates MSE loss between quadratic-transformed relative depths (a·D² +b·D +c) and absolute depth measurements.  \n",
        "- **affine_depth_loss_2d**: Computes MSE loss between linearly-transformed relative depths (a·D +b) and absolute depth values.\n",
        "- **train_in_batches**: Trains a model using RGB-depth batches while optimizing quadratic affine parameters through combined depth-prediction and parameter-proximity losses.  \n",
        "- **filter_by_lighting**: Extracts test dataset entries matching specific lighting conditions from scene description metadata.  \n",
        "- **extract_comparison_stats**: Computes error statistics between model-predicted, polynomial-estimated, and ground-truth depth metrics per bounding box.\n",
        "- **test_yolo_in_batches**: Processes images through YOLO to generate bounding boxes, then compares depth metrics between model-predicted and polynomial-estimated affine parameters for each box.  \n",
        "- **test_in_batches**: Performs batched inference on RGB-depth inputs to predict affine parameters while evaluating depth transformation accuracy against absolute measurements.\n",
        "- **plot_stats_vs_num_points**: Plots the distribution of various depth error metrics as a function of the number of LiDAR points per bounding box.\n",
        "- **plot_comparison_curves**: Creates comparison curves between the trends of the poly and the model, improving the original code with direct comparative visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38yqwHGIjzYt"
      },
      "outputs": [],
      "source": [
        "def scaleDepthTo255(predicted_depth):\n",
        "  depth_m = (predicted_depth - predicted_depth.min()) / (predicted_depth.max() - predicted_depth.min())\n",
        "  depth_m = depth_m.detach().cpu().numpy() * 255\n",
        "  return depth_m\n",
        "\n",
        "def scaleDepthTo255n_np(predicted_depth):\n",
        "  depth_m = (predicted_depth - predicted_depth.min()) / (predicted_depth.max() - predicted_depth.min())\n",
        "  depth_m = depth_m * 255\n",
        "  return depth_m\n",
        "\n",
        "def scaleDepthToIMG(predicted_depth):\n",
        "  depth_m =  scaleDepthTo255(predicted_depth)\n",
        "  depth_img = Image.fromarray(depth_m.astype(\"uint8\"))\n",
        "  return depth_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZ9GwgSQQLb_"
      },
      "outputs": [],
      "source": [
        "def print_comparison_stats(stats):\n",
        "    print(f\"Image Index: {stats.get('image_index', 'N/A')}\")\n",
        "    print(f\"poly_vs_abs_stats_mean: {stats.get('poly_vs_abs_stats_mean', 'N/A'):.4f}\")\n",
        "    print(f\"model_vs_abs_stats_mean: {stats.get('model_vs_abs_stats_mean', 'N/A'):.4f}\")\n",
        "    print(f\"model_vs_poly_stats_mean: {stats.get('model_vs_poly_stats_mean', 'N/A'):.4f}\")\n",
        "    print(f\"poly_vs_abs_per_point_stats_mean: {stats.get('poly_vs_abs_per_point_stats_mean', 'N/A'):.6f}\")\n",
        "    print(f\"model_vs_abs_per_point_stats_mean: {stats.get('model_vs_abs_per_point_stats_mean', 'N/A'):.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IrsHFOmhE6L"
      },
      "outputs": [],
      "source": [
        "def extract_box_depth_data(\n",
        "    depth_rel: np.ndarray,\n",
        "    input_boxes: np.ndarray,\n",
        "    a: float,\n",
        "    b: float = 0.0,\n",
        "    c: float = 0.0,\n",
        "    points_dep_abs: np.ndarray = None,\n",
        "    depths_abs: np.ndarray = None\n",
        ") -> List[Dict]:\n",
        "\n",
        "    depth_r = depth_rel.max() - depth_rel\n",
        "    depth_m = a * depth_r**2 + b * depth_r + c\n",
        "\n",
        "    # Preprocess punti assoluti (se presenti)\n",
        "    xs, ys, deps = None, None, None\n",
        "    if points_dep_abs is not None and depths_abs is not None:\n",
        "        xs, ys = points_dep_abs[0], points_dep_abs[1]\n",
        "        valid = (xs >= 0) & (xs < depth_rel.shape[1]) & (ys >= 0) & (ys < depth_rel.shape[0])\n",
        "        xs, ys, deps = xs[valid], ys[valid], depths_abs[valid]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for box in input_boxes:\n",
        "        x1, y1, x2, y2 = box.astype(int)\n",
        "\n",
        "        # Calcola profondità trasformata\n",
        "        patch = depth_m[y1:y2, x1:x2]\n",
        "        patch = patch[np.isfinite(patch)]\n",
        "        transformed = np.nan if patch.size == 0 else np.mean(patch)\n",
        "\n",
        "        # Inizializza valori assoluti\n",
        "        absolute, count = None, 0\n",
        "\n",
        "        # Calcola punti assoluti nel box (se presenti)\n",
        "        if xs is not None:\n",
        "            in_box = (xs >= x1) & (xs <= x2) & (ys >= y1) & (ys <= y2)\n",
        "            count = in_box.sum()\n",
        "            if count > 0:\n",
        "                absolute = np.mean(deps[in_box])\n",
        "\n",
        "        results.append({\n",
        "            'box': (int(x1), int(y1), int(x2), int(y2)),\n",
        "            'transformed_depth_mean': float(transformed),\n",
        "            'absolute_depth_mean': float(absolute) if absolute is not None else None,\n",
        "            'num_points': int(count)\n",
        "        })\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFzVfJ0wdpla"
      },
      "outputs": [],
      "source": [
        "def plot_with_box_depth_rel_abs(image: np.ndarray,\n",
        "                        depth_rel: np.ndarray,\n",
        "                        input_boxes: np.ndarray,\n",
        "                        a: float,\n",
        "                        b: float = 0.0,\n",
        "                        c: float = 0.0,\n",
        "                        points_dep_abs=None,\n",
        "                        depths_abs=None,\n",
        "                        mask_alpha: float = 0.3,\n",
        "                        text_color: str = \"yellow\",\n",
        "                        box_color: str = \"cyan\",\n",
        "                        abs_points_color: str = \"red\",\n",
        "                        cmap_depth: str = \"plasma\"):\n",
        "\n",
        "    fig, (ax_img, ax_depth) = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "    depth_r = depth_rel.max() - depth_rel\n",
        "    depth_m = a * depth_r**2 + b * depth_r + c\n",
        "    depth_m_255 = scaleDepthTo255n_np(depth_m)  # Assumo sia la tua funzione di scaling\n",
        "\n",
        "    ax_img.imshow(image)\n",
        "    ax_img.axis('off')\n",
        "    ax_img.set_title(\"RGB with Boxes & Depth Information\")\n",
        "\n",
        "    # Visualizza solo punti che cadono in almeno una box\n",
        "    if points_dep_abs is not None and depths_abs is not None:\n",
        "        xs, ys = points_dep_abs[0], points_dep_abs[1]\n",
        "        valid_indices = (xs >= 0) & (xs < depth_rel.shape[1]) & (ys >= 0) & (ys < depth_rel.shape[0])\n",
        "        xs, ys = xs[valid_indices], ys[valid_indices]\n",
        "        depths_abs = depths_abs[valid_indices]\n",
        "\n",
        "        # Usa un array per marcare quali punti sono in almeno una box\n",
        "        points_in_any_box = np.zeros_like(xs, dtype=bool)\n",
        "        for box in input_boxes:\n",
        "            x1, y1, x2, y2 = box.astype(int)\n",
        "            points_in_box = ((xs >= x1) & (xs <= x2) &\n",
        "                            (ys >= y1) & (ys <= y2))\n",
        "            points_in_any_box = points_in_any_box | points_in_box\n",
        "\n",
        "        # Visualizza solo i punti che sono in almeno una box\n",
        "        ax_img.scatter(xs[points_in_any_box], ys[points_in_any_box],\n",
        "                      c=abs_points_color, s=30, alpha=0.8,\n",
        "                      label='Absolute Depth Points (in boxes)')\n",
        "\n",
        "        for box in input_boxes:\n",
        "            x1, y1, x2, y2 = box.astype(int)\n",
        "\n",
        "            # Calcola profondità media dal depth map trasformato\n",
        "            patch = depth_m[y1:y2, x1:x2]\n",
        "            patch = patch[np.isfinite(patch)]\n",
        "            dist_med = np.nan if patch.size == 0 else np.mean(patch)\n",
        "\n",
        "            # Disegna box semi-trasparente\n",
        "            rect = patches.Rectangle((x1, y1),\n",
        "                                    x2 - x1,\n",
        "                                    y2 - y1,\n",
        "                                    linewidth=2,\n",
        "                                    edgecolor=box_color,\n",
        "                                    facecolor=box_color,\n",
        "                                    alpha=mask_alpha)\n",
        "            ax_img.add_patch(rect)\n",
        "\n",
        "            # Testo base per profondità trasformata\n",
        "            text_lines = [f\"Transformed: {dist_med:.2f} m\"]\n",
        "\n",
        "            # Controlla punti di distanza assoluta nel box\n",
        "            if points_dep_abs is not None and depths_abs is not None:\n",
        "                # Qui xs, ys e depths_abs sono già filtrati e validi\n",
        "                points_in_box = ((xs >= x1) & (xs <= x2) &\n",
        "                                (ys >= y1) & (ys <= y2))\n",
        "                if np.any(points_in_box):\n",
        "                    abs_depths_in_box = depths_abs[points_in_box]\n",
        "                    mean_abs_depth = np.mean(abs_depths_in_box)\n",
        "                    num_points = np.sum(points_in_box)\n",
        "                    text_lines.append(f\"Absolute: {mean_abs_depth:.2f} m ({num_points} pts)\\n\\n\")\n",
        "\n",
        "            # Combina le linee di testo\n",
        "            text_content = \"\\n\".join(text_lines)\n",
        "\n",
        "            # STAMPA SU CONSOLE\n",
        "            print(f\"Box at ({x1},{y1})-({x2},{y2}):\\n{text_content}\")\n",
        "\n",
        "            # Annotazione con tutte le informazioni\n",
        "            ax_img.text(x1, y1 - 5,\n",
        "                      text_content,\n",
        "                      color=text_color,\n",
        "                      fontsize=10,\n",
        "                      weight='bold',\n",
        "                      bbox=dict(facecolor='black', alpha=0.6, pad=2),\n",
        "                      verticalalignment='top')\n",
        "\n",
        "\n",
        "            # Aggiungi legenda se ci sono punti assoluti\n",
        "            if points_dep_abs is not None and depths_abs is not None:\n",
        "                ax_img.legend(loc='upper right')\n",
        "\n",
        "    # === Destra: Mappa di profondità ===\n",
        "    im = ax_depth.imshow(depth_m_255,\n",
        "                        cmap=cmap_depth,\n",
        "                        vmin=depth_m_255.min(),\n",
        "                        vmax=depth_m_255.max())\n",
        "    ax_depth.axis('off')\n",
        "    ax_depth.set_title(\"Transformed Depth Map\")\n",
        "\n",
        "    # Aggiungi colorbar\n",
        "    plt.colorbar(im, ax=ax_depth, shrink=0.8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-noaYOTJB653"
      },
      "outputs": [],
      "source": [
        "np.random.seed(3)\n",
        "\n",
        "def plot_with_box_depth(image: np.ndarray,\n",
        "                        depth_rel: np.ndarray,\n",
        "                        input_boxes: np.ndarray,\n",
        "                        a: float,\n",
        "                        b: float = 0.0,\n",
        "                        mask_alpha: float = 0.3,\n",
        "                        text_color: str = \"yellow\",\n",
        "                        box_color: str = \"cyan\",\n",
        "                        cmap_depth: str = \"plasma\"):\n",
        "\n",
        "    # Prepare figure and axes\n",
        "    fig, (ax_img, ax_depth) = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "    depth_r = depth_rel.max() - depth_rel\n",
        "    depth_m = a * depth_r + b\n",
        "    depth_m_255 = scaleDepthTo255n_np(depth_m)\n",
        "\n",
        "\n",
        "\n",
        "    # === Left: RGB + boxes + mean depth annotation ===\n",
        "    ax_img.imshow(image)\n",
        "    ax_img.axis('off')\n",
        "    ax_img.set_title(\"RGB with Boxes & Mean Depth\")\n",
        "\n",
        "    for box in input_boxes:\n",
        "        x1, y1, x2, y2 = box.astype(int)\n",
        "\n",
        "        # Compute mean depth inside box\n",
        "        patch = depth_m[y1:y2, x1:x2]\n",
        "        patch = patch[np.isfinite(patch)]\n",
        "        dist_med = np.nan if patch.size == 0 else np.mean(patch)\n",
        "\n",
        "        # Draw semi-transparent box\n",
        "        rect = patches.Rectangle((x1, y1),\n",
        "                                 x2 - x1,\n",
        "                                 y2 - y1,\n",
        "                                 linewidth=2,\n",
        "                                 edgecolor=box_color,\n",
        "                                 facecolor=box_color,\n",
        "                                 alpha=mask_alpha)\n",
        "        ax_img.add_patch(rect)\n",
        "\n",
        "        # Annotate mean depth\n",
        "        ax_img.text(x1, y1 - 5,\n",
        "                    f\"{dist_med:.2f} m\",\n",
        "                    color=text_color,\n",
        "                    fontsize=12,\n",
        "                    weight='bold',\n",
        "                    bbox=dict(facecolor='black', alpha=0.4, pad=2))\n",
        "\n",
        "    # === Right: Depth map ===\n",
        "    im = ax_depth.imshow(depth_m_255,\n",
        "                         cmap=cmap_depth,\n",
        "                         vmin=depth_m_255.min(),\n",
        "                         vmax=depth_m_255.max())\n",
        "    ax_depth.axis('off')\n",
        "    ax_depth.set_title(\"Depth Map\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_with_box_depth_3d(image: np.ndarray,\n",
        "                        depth_rel: np.ndarray,\n",
        "                        input_boxes: np.ndarray,\n",
        "                        a: float,\n",
        "                        b: float = 0.0,\n",
        "                        c: float = 0.0,\n",
        "                        mask_alpha: float = 0.3,\n",
        "                        text_color: str = \"yellow\",\n",
        "                        box_color: str = \"cyan\",\n",
        "                        cmap_depth: str = \"plasma\"):\n",
        "\n",
        "    # Prepare figure and axes\n",
        "    fig, (ax_img, ax_depth) = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "    depth_r = depth_rel.max() - depth_rel\n",
        "    depth_m = a * depth_r**2 + b * depth_r + c\n",
        "    depth_m_255 = scaleDepthTo255n_np(depth_m)\n",
        "\n",
        "\n",
        "\n",
        "    # === Left: RGB + boxes + mean depth annotation ===\n",
        "    ax_img.imshow(image)\n",
        "    ax_img.axis('off')\n",
        "    ax_img.set_title(\"RGB with Boxes & Mean Depth\")\n",
        "\n",
        "    for box in input_boxes:\n",
        "        x1, y1, x2, y2 = box.astype(int)\n",
        "\n",
        "        # Compute mean depth inside box\n",
        "        patch = depth_m[y1:y2, x1:x2]\n",
        "        patch = patch[np.isfinite(patch)]\n",
        "        dist_med = np.nan if patch.size == 0 else np.mean(patch)\n",
        "\n",
        "        # Draw semi-transparent box\n",
        "        rect = patches.Rectangle((x1, y1),\n",
        "                                 x2 - x1,\n",
        "                                 y2 - y1,\n",
        "                                 linewidth=2,\n",
        "                                 edgecolor=box_color,\n",
        "                                 facecolor=box_color,\n",
        "                                 alpha=mask_alpha)\n",
        "        ax_img.add_patch(rect)\n",
        "\n",
        "        # Annotate mean depth\n",
        "        ax_img.text(x1, y1 - 5,\n",
        "                    f\"{dist_med:.2f} m\",\n",
        "                    color=text_color,\n",
        "                    fontsize=12,\n",
        "                    weight='bold',\n",
        "                    bbox=dict(facecolor='black', alpha=0.4, pad=2))\n",
        "\n",
        "    # === Right: Depth map ===\n",
        "    im = ax_depth.imshow(depth_m_255,\n",
        "                         cmap=cmap_depth,\n",
        "                         vmin=depth_m_255.min(),\n",
        "                         vmax=depth_m_255.max())\n",
        "    ax_depth.axis('off')\n",
        "    ax_depth.set_title(\"Depth Map\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def map_pointcloud_to_depth(pointsensor_token: str,\n",
        "                                camera_token: str,\n",
        "                                min_dist: float = 1.0,\n",
        "                                render_intensity: bool = False,\n",
        "                                cmap: str = 'viridis',\n",
        "                                cnorm: bool = True) -> Tuple:\n",
        "\n",
        "\n",
        "        cam = trucksc.get('sample_data', camera_token)\n",
        "        pointsensor = trucksc.get('sample_data', pointsensor_token)\n",
        "        pcl_path = osp.join(trucksc.dataroot, pointsensor['filename'])\n",
        "        if pointsensor['sensor_modality'] == 'lidar':\n",
        "            pc = LidarPointCloud.from_file(pcl_path)\n",
        "        else:\n",
        "            pc = RadarPointCloud.from_file(pcl_path)\n",
        "        im = Image.open(osp.join(trucksc.dataroot, cam['filename']))\n",
        "\n",
        "        cs_record = trucksc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])\n",
        "        pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)\n",
        "        pc.translate(np.array(cs_record['translation']))\n",
        "\n",
        "        poserecord = trucksc.get('ego_pose', pointsensor['ego_pose_token'])\n",
        "        pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix)\n",
        "        pc.translate(np.array(poserecord['translation']))\n",
        "\n",
        "        poserecord = trucksc.get('ego_pose', cam['ego_pose_token'])\n",
        "        pc.translate(-np.array(poserecord['translation']))\n",
        "        pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix.T)\n",
        "\n",
        "        cs_record = trucksc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
        "        pc.translate(-np.array(cs_record['translation']))\n",
        "        pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix.T)\n",
        "\n",
        "        depths = pc.points[2, :]\n",
        "\n",
        "        if render_intensity:\n",
        "            if pointsensor['sensor_modality'] == 'lidar':\n",
        "                coloring = pc.points[3, :]\n",
        "            else:\n",
        "                coloring = pc.points[6, :]\n",
        "        else:\n",
        "            coloring = depths\n",
        "\n",
        "        if cnorm:\n",
        "            norm = Normalize(vmin=np.quantile(coloring, 0.5),\n",
        "                             vmax=np.quantile(coloring, 0.95), clip=True)\n",
        "        else:\n",
        "            norm = None\n",
        "        mapper = ScalarMappable(norm=norm, cmap=cmap)\n",
        "        coloring = mapper.to_rgba(coloring)[..., :3]\n",
        "\n",
        "        points = view_points(pc.points[:3, :], np.array(cs_record['camera_intrinsic']),\n",
        "                             normalize=True)\n",
        "\n",
        "\n",
        "        mask = np.ones(depths.shape[0], dtype=bool)\n",
        "        mask = np.logical_and(mask, depths > min_dist)\n",
        "        mask = np.logical_and(mask, points[0, :] > 1)\n",
        "        mask = np.logical_and(mask, points[0, :] < im.size[0] - 1)\n",
        "        mask = np.logical_and(mask, points[1, :] > 1)\n",
        "        mask = np.logical_and(mask, points[1, :] < im.size[1] - 1)\n",
        "\n",
        "        points = points[:, mask]\n",
        "        coloring = coloring[mask, :]\n",
        "        depths = depths[mask]\n",
        "\n",
        "        return points, coloring, im , depths\n",
        "\n",
        "def render_pointcloud_in_deapth(\n",
        "                                   sample_token: str,\n",
        "                                   pointsensor_channel: str = 'LIDAR_LEFT',\n",
        "                                   camera_channel: str = 'CAMERA_LEFT_FRONT',\n",
        "                                   render_intensity: bool = False,\n",
        "                                   cmap: str = 'viridis'):\n",
        "\n",
        "\n",
        "\n",
        "        sample_record = trucksc.get('sample', sample_token)\n",
        "\n",
        "        pointsensor_token = sample_record['data'][pointsensor_channel]\n",
        "        camera_token = sample_record['data'][camera_channel]\n",
        "\n",
        "        points, coloring, im, depth = map_pointcloud_to_depth(pointsensor_token, camera_token,\n",
        "                                                            render_intensity=render_intensity,\n",
        "                                                            cmap=cmap)\n",
        "\n",
        "        return points, coloring, im, depth\n",
        "\n",
        "\n",
        "def show_anns_on_ax(anns: List[Dict], ax: plt.Axes, borders: bool = True):\n",
        "    if not anns:\n",
        "        return\n",
        "    sorted_anns = sorted(anns, key=lambda x: x['area'], reverse=True)\n",
        "\n",
        "    h, w = sorted_anns[0]['segmentation'].shape\n",
        "    overlay = np.zeros((h, w, 4), dtype=float)\n",
        "\n",
        "    for ann in sorted_anns:\n",
        "        m = ann['segmentation'].astype(bool)\n",
        "        color = np.concatenate([np.random.rand(3), [0.5]])  # rgba\n",
        "        overlay[m] = color\n",
        "\n",
        "        if borders:\n",
        "            # Estrai i contorni e li disegna\n",
        "            contours, _ = cv2.findContours(\n",
        "                m.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE\n",
        "            )\n",
        "            smooth = [cv2.approxPolyDP(c, 0.01 * cv2.arcLength(c, True), True)\n",
        "                      for c in contours]\n",
        "            cv2.drawContours(overlay, smooth, -1, (0,0,1,0.8), 1)\n",
        "\n",
        "    ax.imshow(overlay)\n",
        "\n",
        "\n",
        "def plot_with_box_depth2(image: np.ndarray,\n",
        "                        depth_rel: np.ndarray,\n",
        "                        input_boxes: np.ndarray,\n",
        "                        a: float,\n",
        "                        b: float = 0.0,\n",
        "                        mask_alpha: float = 0.3,\n",
        "                        text_color: str = \"yellow\",\n",
        "                        box_color: str = \"cyan\"):\n",
        "\n",
        "\n",
        "    # Calibro la profondità\n",
        "\n",
        "    depth_r = depth_rel.max() - depth_rel\n",
        "\n",
        "    depth_m = a * depth_r + b\n",
        "\n",
        "    # Preparo figura ad un solo asse\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(16, 12))\n",
        "    ax.imshow(image)\n",
        "    ax.axis('off')\n",
        "    ax.set_title(\"RGB with Boxes & Mean Depth\")\n",
        "\n",
        "    for box in input_boxes:\n",
        "        x1, y1, x2, y2 = box.astype(int)\n",
        "\n",
        "        # Calcolo distanza media nel box\n",
        "        patch = depth_m[y1:y2, x1:x2]\n",
        "        patch = patch[np.isfinite(patch)]\n",
        "        dist_med = np.nan if patch.size == 0 else np.mean(patch)\n",
        "\n",
        "        # Disegno box semi‐trasparente\n",
        "        rect = patches.Rectangle((x1, y1),\n",
        "                                 x2 - x1,\n",
        "                                 y2 - y1,\n",
        "                                 linewidth=2,\n",
        "                                 edgecolor=box_color,\n",
        "                                 facecolor=box_color,\n",
        "                                 alpha=mask_alpha)\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Annotazione distanza\n",
        "        ax.text(x1, y1 - 5,\n",
        "                f\"{dist_med:.2f} m\",\n",
        "                color=text_color,\n",
        "                fontsize=12,\n",
        "                weight='bold',\n",
        "                bbox=dict(facecolor='black', alpha=0.4, pad=2))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_with_box_depth3(image: np.ndarray,\n",
        "                        depth_rel: np.ndarray,\n",
        "                        input_boxes: np.ndarray,\n",
        "                        a: float,\n",
        "                        b: float = 0.0,\n",
        "                        c: float = 0.0,\n",
        "                        mask_alpha: float = 0.3,\n",
        "                        text_color: str = \"yellow\",\n",
        "                        box_color: str = \"cyan\"):\n",
        "\n",
        "\n",
        "    # Calibro la profondità\n",
        "\n",
        "    depth_r = depth_rel.max() - depth_rel\n",
        "\n",
        "    depth_m = a * depth_r**2  + b * depth_r + c\n",
        "\n",
        "    # Preparo figura ad un solo asse\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(16, 12))\n",
        "    ax.imshow(image)\n",
        "    ax.axis('off')\n",
        "    ax.set_title(\"RGB with Boxes & Mean Depth\")\n",
        "\n",
        "    for box in input_boxes:\n",
        "        x1, y1, x2, y2 = box.astype(int)\n",
        "\n",
        "        # Calcolo distanza media nel box\n",
        "        patch = depth_m[y1:y2, x1:x2]\n",
        "        patch = patch[np.isfinite(patch)]\n",
        "        dist_med = np.nan if patch.size == 0 else np.mean(patch)\n",
        "\n",
        "        # Disegno box semi‐trasparente\n",
        "        rect = patches.Rectangle((x1, y1),\n",
        "                                 x2 - x1,\n",
        "                                 y2 - y1,\n",
        "                                 linewidth=2,\n",
        "                                 edgecolor=box_color,\n",
        "                                 facecolor=box_color,\n",
        "                                 alpha=mask_alpha)\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Annotazione distanza\n",
        "        ax.text(x1, y1 - 5,\n",
        "                f\"{dist_med:.2f} m\",\n",
        "                color=text_color,\n",
        "                fontsize=12,\n",
        "                weight='bold',\n",
        "                bbox=dict(facecolor='black', alpha=0.4, pad=2))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWwjmdSmBbqz"
      },
      "outputs": [],
      "source": [
        "def plot_comparison_curves(comparison_stats_boxes):\n",
        "    # Estrazione dati e filtraggio valori validi\n",
        "    num_points = []\n",
        "    poly_vs_abs = []\n",
        "    model_vs_abs = []\n",
        "    poly_vs_abs_per_point = []\n",
        "    model_vs_abs_per_point = []\n",
        "\n",
        "    for box in comparison_stats_boxes:\n",
        "        if (box['num_points'] is not None and box['num_points'] > 0 and\n",
        "            box['poly_vs_abs'] is not None and box['model_vs_abs'] is not None and\n",
        "            box['poly_vs_abs_per_point'] is not None and box['model_vs_abs_per_point'] is not None):\n",
        "            num_points.append(box['num_points'])\n",
        "            poly_vs_abs.append(box['poly_vs_abs'])\n",
        "            model_vs_abs.append(box['model_vs_abs'])\n",
        "            poly_vs_abs_per_point.append(box['poly_vs_abs_per_point'])\n",
        "            model_vs_abs_per_point.append(box['model_vs_abs_per_point'])\n",
        "\n",
        "    num_points = np.array(num_points)\n",
        "    poly_vs_abs = np.array(poly_vs_abs)\n",
        "    model_vs_abs = np.array(model_vs_abs)\n",
        "    poly_vs_abs_per_point = np.array(poly_vs_abs_per_point)\n",
        "    model_vs_abs_per_point = np.array(model_vs_abs_per_point)\n",
        "\n",
        "    # Rimozione outlier e valori estremi per una migliore leggibilità\n",
        "    max_error = np.percentile(np.concatenate([poly_vs_abs, model_vs_abs]), 99)\n",
        "    mask = (poly_vs_abs < max_error) & (model_vs_abs < max_error)\n",
        "    num_points = num_points[mask]\n",
        "    poly_vs_abs = poly_vs_abs[mask]\n",
        "    model_vs_abs = model_vs_abs[mask]\n",
        "    poly_vs_abs_per_point = poly_vs_abs_per_point[mask]\n",
        "    model_vs_abs_per_point = model_vs_abs_per_point[mask]\n",
        "\n",
        "    # Figura 2x2\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    fig.suptitle('Performance Comparison: Poly vs Model', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # 1. Errore assoluto vs numero di punti\n",
        "    axs[0,0].scatter(num_points, poly_vs_abs, alpha=0.4, label='Poly', color='blue', s=20)\n",
        "    axs[0,0].scatter(num_points, model_vs_abs, alpha=0.4, label='Model', color='orange', s=20)\n",
        "    axs[0,0].set_title('Absolute Error vs Number of Points')\n",
        "    axs[0,0].set_xlabel('Number of Points')\n",
        "    axs[0,0].set_ylabel('Absolute Error (m)')\n",
        "    axs[0,0].legend()\n",
        "    axs[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Differenza Model - Poly vs numero di punti\n",
        "    difference = model_vs_abs - poly_vs_abs\n",
        "    axs[0,1].scatter(num_points, difference, c=np.where(difference > 0, 'red', 'green'), alpha=0.5, s=20)\n",
        "    axs[0,1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "    axs[0,1].set_title('Difference (Model - Poly) vs Number of Points')\n",
        "    axs[0,1].set_xlabel('Number of Points')\n",
        "    axs[0,1].set_ylabel('Error Difference (m)')\n",
        "    axs[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Errore normalizzato per punto vs numero di punti (log scale per X)\n",
        "    axs[1,0].scatter(num_points, poly_vs_abs_per_point, alpha=0.4, label='Poly/point', color='blue', s=20)\n",
        "    axs[1,0].scatter(num_points, model_vs_abs_per_point, alpha=0.4, label='Model/point', color='orange', s=20)\n",
        "    axs[1,0].set_title('Normalized Error per Point')\n",
        "    axs[1,0].set_xlabel('Number of Points')\n",
        "    axs[1,0].set_ylabel('Error per Point (m)')\n",
        "    axs[1,0].set_yscale('log')\n",
        "    axs[1,0].legend()\n",
        "    axs[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Rapporto di efficienza (Poly/Model) vs numero di punti\n",
        "    ratio = np.divide(poly_vs_abs, model_vs_abs, out=np.full_like(poly_vs_abs, np.nan), where=model_vs_abs!=0)\n",
        "    ratio = np.clip(ratio, 0, 5)  # Limita i valori estremi per leggibilità\n",
        "    axs[1,1].scatter(num_points, ratio, alpha=0.5, color='purple', s=20)\n",
        "    axs[1,1].axhline(y=1, color='black', linestyle='--', alpha=0.7, label='Parity')\n",
        "    axs[1,1].set_title('Efficiency Ratio (Poly/Model)')\n",
        "    axs[1,1].set_xlabel('Number of Points')\n",
        "    axs[1,1].set_ylabel('Error Ratio')\n",
        "    axs[1,1].legend()\n",
        "    axs[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "        # Quantitative analysis printout\n",
        "    print(\"=== QUANTITATIVE ANALYSIS ===\")\n",
        "    print(f\"Total number of observations: {len(num_points)}\")\n",
        "    print(f\"Range of number of points: {num_points.min()} - {num_points.max()}\")\n",
        "    print()\n",
        "\n",
        "    print(\"Poly Statistics:\")\n",
        "    print(f\"  Mean error: {poly_vs_abs.mean():.3f} ± {poly_vs_abs.std():.3f} m\")\n",
        "    print(f\"  Mean error per point: {poly_vs_abs_per_point.mean():.6f} m\")\n",
        "    print()\n",
        "\n",
        "    print(\"Model Statistics:\")\n",
        "    print(f\"  Mean error: {model_vs_abs.mean():.3f} ± {model_vs_abs.std():.3f} m\")\n",
        "    print(f\"  Mean error per point: {model_vs_abs_per_point.mean():.6f} m\")\n",
        "    print()\n",
        "\n",
        "    print(\"Comparison:\")\n",
        "    better_poly = np.sum(poly_vs_abs < model_vs_abs)\n",
        "    better_model = np.sum(model_vs_abs < poly_vs_abs)\n",
        "    print(f\"  Poly better in {better_poly}/{len(num_points)} cases ({better_poly/len(num_points)*100:.1f}%)\")\n",
        "    print(f\"  Model better in {better_model}/{len(num_points)} cases ({better_model/len(num_points)*100:.1f}%)\")\n",
        "\n",
        "    # Analysis by number of points ranges\n",
        "    print(\"\\nAnalysis by ranges:\")\n",
        "    small_mask = num_points <= 100\n",
        "    medium_mask = (num_points > 100) & (num_points <= 300)\n",
        "    large_mask = num_points > 300\n",
        "\n",
        "    for mask, name in [(small_mask, \"Small (≤100)\"), (medium_mask, \"Medium (101-300)\"), (large_mask, \"Large (>300)\")]:\n",
        "        if np.any(mask):\n",
        "            poly_mean = poly_vs_abs[mask].mean()\n",
        "            model_mean = model_vs_abs[mask].mean()\n",
        "            print(f\"  {name}: Poly={poly_mean:.3f}m, Model={model_mean:.3f}m, Diff={model_mean-poly_mean:.3f}m\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVxuyrC89EDe"
      },
      "outputs": [],
      "source": [
        "def affine_param_loss(a_b_c_hat, a_b_c, delta=1.0, device='cuda'):\n",
        "    # Converti l'array NumPy a tensor PyTorch e portalo sullo stesso device\n",
        "    a_b_c_tensor = torch.as_tensor(a_b_c, dtype=torch.float32, device=device)\n",
        "\n",
        "    # Calcola la differenza\n",
        "    diff = a_b_c_hat - a_b_c_tensor\n",
        "\n",
        "    # Huber loss\n",
        "    loss = torch.where(\n",
        "        torch.abs(diff) < delta,\n",
        "        0.5 * diff ** 2,\n",
        "        delta * (torch.abs(diff) - 0.5 * delta)\n",
        "    )\n",
        "\n",
        "    return torch.mean(torch.sum(loss, dim=1))\n",
        "\n",
        "def affine_depth_loss(a_b_c_hat, depths_rel, points_dep_abs, depths_asb):\n",
        "    batch_size = a_b_c_hat.size(0)\n",
        "    D_preds = []\n",
        "    D_asbs = []\n",
        "\n",
        "    # Iterate through each item in the batch\n",
        "    for i in range(batch_size):\n",
        "        a_i = a_b_c_hat[i, 0]  # Get the predicted 'a' for the i-th item\n",
        "        b_i = a_b_c_hat[i, 1]  # Get the predicted 'b' for the i-th item\n",
        "        c_i = a_b_c_hat[i, 2]  # Get the predicted 'b' for the i-th item\n",
        "\n",
        "        depth_rel_img = depths_rel[i]\n",
        "        points = points_dep_abs[i]\n",
        "        depth_abs = depths_asb[i]\n",
        "\n",
        "        xs = points[0].astype(int)\n",
        "        ys = points[1].astype(int)\n",
        "\n",
        "        depth_rel_inv = depth_rel_img\n",
        "        # Ensure depth_rel is on the same device as a_b_hat for calculations\n",
        "        depth_rel = (depth_rel_inv.max() - depth_rel_inv).to(a_b_c_hat.device)\n",
        "\n",
        "        # Index depth_rel using point coordinates\n",
        "        # Need to handle potential out-of-bounds indexing\n",
        "        h, w = depth_rel.shape\n",
        "        valid_indices = (xs >= 0) & (xs < w) & (ys >= 0) & (ys < h)\n",
        "\n",
        "        xs_valid = xs[valid_indices]\n",
        "        ys_valid = ys[valid_indices]\n",
        "        Drel = depth_rel[ys_valid, xs_valid]\n",
        "\n",
        "        Dabs_tensor = torch.from_numpy(depth_abs[valid_indices]).float().to(a_b_c_hat.device)\n",
        "\n",
        "        # Apply the affine transformation\n",
        "        D_pred = a_i * Drel**2 + b_i * Drel + c_i\n",
        "\n",
        "        D_preds.append(D_pred)\n",
        "        D_asbs.append(Dabs_tensor)\n",
        "\n",
        "    # Concatenate the predicted and absolute depth tensors from all items in the batch\n",
        "    # Filter out empty tensors if some items had no valid points\n",
        "    D_preds_cat = torch.cat([p for p in D_preds if p.numel() > 0])\n",
        "    D_asbs_cat = torch.cat([a for a in D_asbs if a.numel() > 0])\n",
        "\n",
        "    # Calculate the loss (only if there are valid points across the batch)\n",
        "    if D_preds_cat.numel() > 0:\n",
        "        loss = torch.mean((torch.abs(D_preds_cat - D_asbs_cat))**2)\n",
        "    else:\n",
        "        # Return a zero loss or handle appropriately if no valid points are found in a batch\n",
        "        loss = torch.tensor(0.0, device=a_b_c_hat.device)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def affine_depth_loss_2d(a_b_hat, depths_rel, points_dep_abs, depths_asb):\n",
        "    batch_size = a_b_hat.size(0)\n",
        "    D_preds = []\n",
        "    D_asbs = []\n",
        "\n",
        "    # Iterate through each item in the batch\n",
        "    for i in range(batch_size):\n",
        "        a_i = a_b_hat[i, 0]  # Get the predicted 'a' for the i-th item\n",
        "        b_i = a_b_hat[i, 1]  # Get the predicted 'b' for the i-th item\n",
        "\n",
        "        depth_rel_img = depths_rel[i]\n",
        "        points = points_dep_abs[i]\n",
        "        depth_abs = depths_asb[i]\n",
        "\n",
        "        xs = points[0].astype(int)\n",
        "        ys = points[1].astype(int)\n",
        "\n",
        "        depth_rel_inv = depth_rel_img\n",
        "        # Ensure depth_rel is on the same device as a_b_hat for calculations\n",
        "        depth_rel = (depth_rel_inv.max() - depth_rel_inv).to(a_b_hat.device)\n",
        "\n",
        "        # Index depth_rel using point coordinates\n",
        "        # Need to handle potential out-of-bounds indexing\n",
        "        h, w = depth_rel.shape\n",
        "        valid_indices = (xs >= 0) & (xs < w) & (ys >= 0) & (ys < h)\n",
        "\n",
        "        xs_valid = xs[valid_indices]\n",
        "        ys_valid = ys[valid_indices]\n",
        "        Drel = depth_rel[ys_valid, xs_valid]\n",
        "\n",
        "        Dabs_tensor = torch.from_numpy(depth_abs[valid_indices]).float().to(a_b_hat.device)\n",
        "\n",
        "        # Apply the affine transformation\n",
        "        D_pred = a_i * Drel + b_i\n",
        "\n",
        "        D_preds.append(D_pred)\n",
        "        D_asbs.append(Dabs_tensor)\n",
        "\n",
        "    # Concatenate the predicted and absolute depth tensors from all items in the batch\n",
        "    # Filter out empty tensors if some items had no valid points\n",
        "    D_preds_cat = torch.cat([p for p in D_preds if p.numel() > 0])\n",
        "    D_asbs_cat = torch.cat([a for a in D_asbs if a.numel() > 0])\n",
        "\n",
        "    # Calculate the loss (only if there are valid points across the batch)\n",
        "    if D_preds_cat.numel() > 0:\n",
        "        loss = torch.mean((torch.abs(D_preds_cat - D_asbs_cat))**2)\n",
        "    else:\n",
        "        # Return a zero loss or handle appropriately if no valid points are found in a batch\n",
        "        loss = torch.tensor(0.0, device=a_b_hat.device)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZtynI-kf7Yh"
      },
      "outputs": [],
      "source": [
        "def train_in_batches(cam_images, depths_rel, points_dep_abs, depths_asb, param_poly,\n",
        "                     cam_images_test, depths_rel_test, points_dep_abs_test, depths_asb_test,\n",
        "                     model, optimizer, num_epochs=10, batch_size=64, device='cpu',patience=5):\n",
        "    transform = ToTensor()\n",
        "    model.to(device)\n",
        "\n",
        "    total_samples = len(cam_images)\n",
        "    min_delta = 0.001\n",
        "    best_loss = float('inf')\n",
        "    counter = 0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        shuffled_indices = np.random.permutation(total_samples)\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i in range(0, total_samples, batch_size):\n",
        "            batch_indices = shuffled_indices[i:i+batch_size]\n",
        "            batch_rgb_images = [cam_images[idx] for idx in batch_indices]\n",
        "            batch_depths_rel = [depths_rel[idx] for idx in batch_indices]\n",
        "            batch_points_dep_abs = [points_dep_abs[idx] for idx in batch_indices]\n",
        "            batch_depths_asb = [depths_asb[idx] for idx in batch_indices]\n",
        "            batch_param_poly = [param_poly[idx] for idx in batch_indices]\n",
        "\n",
        "            rgb_tensors = [transform(img).to(device) for img in batch_rgb_images]\n",
        "            rgb_batch = torch.stack(rgb_tensors, dim=0)\n",
        "\n",
        "            d_rel_tensors = [depth_rel.float().to(device) for depth_rel in batch_depths_rel]\n",
        "            try:\n",
        "                d_rel_batch = torch.stack(d_rel_tensors, dim=0).unsqueeze(1)\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Error stacking depth tensors at batch index {i}: {e}\")\n",
        "                print(\"Tensor sizes in batch:\", [t.size() for t in d_rel_tensors])\n",
        "                continue\n",
        "            input_tensor = torch.cat([rgb_batch, d_rel_batch], dim=1)\n",
        "\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            a_b_c_hat = model(input_tensor)\n",
        "            loss_depth = affine_depth_loss(a_b_c_hat, batch_depths_rel, batch_points_dep_abs, batch_depths_asb)\n",
        "            loss_param = affine_param_loss(a_b_c_hat, batch_param_poly)\n",
        "            loss = loss_depth + 0.3 * loss_param\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * len(batch_rgb_images)\n",
        "\n",
        "            if device != 'cpu':\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        epoch_loss = running_loss / total_samples\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        _, test_loss = test_in_batches(\n",
        "            cam_images_test, depths_rel_test, points_dep_abs_test, depths_asb_test,\n",
        "            model, batch_size=1, device=device\n",
        "        )\n",
        "        print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "        # Early stopping logic\n",
        "        if test_loss < best_loss - min_delta:\n",
        "            best_loss = test_loss\n",
        "            counter = 0\n",
        "            best_state = model.state_dict()\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                if best_state is not None:\n",
        "                    model.load_state_dict(best_state)\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yLMFLC18YkW"
      },
      "outputs": [],
      "source": [
        "def filter_by_lighting(lighting_value, cam_images_test, depths_rel_test, desc_test,\n",
        "                       points_dep_abs_test, depths_asb_test, params_depth_test, params_depth_test_3d,a_b_c):\n",
        "    indices = []\n",
        "    for i, desc in enumerate(desc_test):\n",
        "        # Cerca il valore di lighting nella stringa di descrizione\n",
        "        for part in desc.split(';'):\n",
        "            if part.startswith('lighting.'):\n",
        "                if part.split('.')[1] == lighting_value:\n",
        "                    indices.append(i)\n",
        "                break\n",
        "    # Estrai i dati relativi solo agli indici trovati\n",
        "    cam_images_sel = [cam_images_test[i] for i in indices]\n",
        "    depths_rel_sel = [depths_rel_test[i] for i in indices]\n",
        "    desc_sel = [desc_test[i] for i in indices]\n",
        "    points_dep_abs_sel = [points_dep_abs_test[i] for i in indices]\n",
        "    depths_asb_sel = [depths_asb_test[i] for i in indices]\n",
        "    params_depth_sel = [params_depth_test[i] for i in indices]\n",
        "    params_depth_sel_3d = [params_depth_test_3d[i] for i in indices]\n",
        "    a_b_c_sel = [a_b_c[i] for i in indices]\n",
        "    return (cam_images_sel, depths_rel_sel, desc_sel,\n",
        "            points_dep_abs_sel, depths_asb_sel, params_depth_sel, params_depth_sel_3d,a_b_c_sel)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH3DEEQqQ_Q7"
      },
      "outputs": [],
      "source": [
        "def calculate_means(comparison_stats):\n",
        "    exclude_keys = ['image_index', 'box_stats']\n",
        "    sums = {}\n",
        "    counts = {}\n",
        "    for entry in comparison_stats:\n",
        "        for key, value in entry.items():\n",
        "            if key not in exclude_keys and value is not None:\n",
        "                if key not in sums:\n",
        "                    sums[key] = 0.0\n",
        "                    counts[key] = 0\n",
        "                sums[key] += value\n",
        "                counts[key] += 1\n",
        "    means = {key: (sums[key] / counts[key] if counts[key] > 0 else None) for key in sums}\n",
        "    return means\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwY_m4OZ7_OE"
      },
      "outputs": [],
      "source": [
        "def extract_comparison_stats(outputs):\n",
        "    comparison_stats = []\n",
        "    comparison_stats_boxes = []\n",
        "\n",
        "    for img_result in outputs:\n",
        "        model_data = img_result['model_data']\n",
        "        poly_data = img_result['poly_data']\n",
        "        poly_vs_abs_stats = []\n",
        "        model_vs_abs_stats = []\n",
        "        model_vs_poly_stats = []\n",
        "        poly_vs_abs_per_point_stats = []\n",
        "        model_vs_abs_per_point_stats = []\n",
        "        model_vs_poly_per_point_stats = []\n",
        "\n",
        "        if not model_data or not poly_data:\n",
        "            continue\n",
        "\n",
        "        min_len = min(len(model_data), len(poly_data))\n",
        "        if min_len == 0:\n",
        "            continue\n",
        "\n",
        "        img_stats = []\n",
        "        for i in range(min_len):\n",
        "            box_model = model_data[i]\n",
        "            box_poly = poly_data[i]\n",
        "\n",
        "            # Prendi solo i box con coordinate uguali (per sicurezza)\n",
        "            if box_model['box'] != box_poly['box']:\n",
        "                print(f\"Warning: box non corrispondenti in immagine {img_result['image_index']}\")\n",
        "                continue\n",
        "\n",
        "            diff_transformed = None\n",
        "            if (box_model['transformed_depth_mean'] is not None and\n",
        "                box_poly['transformed_depth_mean'] is not None):\n",
        "                diff_transformed = box_model['transformed_depth_mean'] - box_poly['transformed_depth_mean']\n",
        "                model_vs_poly_stats.append(diff_transformed)\n",
        "\n",
        "            poly_vs_abs = None\n",
        "            model_vs_abs = None\n",
        "\n",
        "            if (box_poly['transformed_depth_mean'] is not None and\n",
        "                box_model['absolute_depth_mean'] is not None):\n",
        "                poly_vs_abs = box_poly['transformed_depth_mean'] - box_model['absolute_depth_mean']\n",
        "                poly_vs_abs_stats.append(poly_vs_abs)\n",
        "            if (box_model['transformed_depth_mean'] is not None and\n",
        "                box_model['absolute_depth_mean'] is not None):\n",
        "                model_vs_abs = box_model['transformed_depth_mean'] - box_model['absolute_depth_mean']\n",
        "                model_vs_abs_stats.append(model_vs_abs)\n",
        "\n",
        "            poly_vs_abs_per_point = poly_vs_abs / box_model['num_points'] if box_model['num_points'] > 0 and poly_vs_abs is not None else None\n",
        "            model_vs_abs_per_point = model_vs_abs / box_model['num_points'] if box_model['num_points'] > 0 and model_vs_abs is not None else None\n",
        "            diff_transformed_per_point = diff_transformed / box_model['num_points'] if box_model['num_points'] > 0 and diff_transformed is not None else None\n",
        "\n",
        "            poly_vs_abs_per_point_stats.append(poly_vs_abs_per_point)\n",
        "            model_vs_abs_per_point_stats.append(model_vs_abs_per_point)\n",
        "            model_vs_poly_per_point_stats.append(diff_transformed_per_point)\n",
        "\n",
        "            img_stats.append({\n",
        "                'box': box_model['box'],\n",
        "                'poly_vs_abs': poly_vs_abs,\n",
        "                'model_vs_abs': model_vs_abs,\n",
        "                'diff_transformed': diff_transformed,\n",
        "                'num_points': box_model['num_points'],\n",
        "                'poly_vs_abs_per_point': poly_vs_abs_per_point,\n",
        "                'model_vs_abs_per_point': model_vs_abs_per_point,\n",
        "                'diff_transformed_per_point': diff_transformed_per_point,\n",
        "            })\n",
        "            comparison_stats_boxes.append({\n",
        "                'box': box_model['box'],\n",
        "                'poly_vs_abs': poly_vs_abs,\n",
        "                'model_vs_abs': model_vs_abs,\n",
        "                'diff_transformed': diff_transformed,\n",
        "                'num_points': box_model['num_points'],\n",
        "                'poly_vs_abs_per_point': poly_vs_abs_per_point,\n",
        "                'model_vs_abs_per_point': model_vs_abs_per_point,\n",
        "                'diff_transformed_per_point': diff_transformed_per_point,\n",
        "            })\n",
        "\n",
        "        comparison_stats.append({\n",
        "            'image_index': img_result['image_index'],\n",
        "            'poly_vs_abs_stats_mean': np.mean([x for x in poly_vs_abs_stats if x is not None]) if poly_vs_abs_stats else None,\n",
        "            'model_vs_abs_stats_mean': np.mean([x for x in model_vs_abs_stats if x is not None]) if model_vs_abs_stats else None,\n",
        "            'model_vs_poly_stats_mean': np.mean([x for x in model_vs_poly_stats if x is not None]) if model_vs_poly_stats else None,\n",
        "            'poly_vs_abs_per_point_stats_mean': np.mean([x for x in poly_vs_abs_per_point_stats if x is not None]) if poly_vs_abs_per_point_stats else None,\n",
        "            'model_vs_abs_per_point_stats_mean': np.mean([x for x in model_vs_abs_per_point_stats if x is not None]) if model_vs_abs_per_point_stats else None,\n",
        "            'box_stats': img_stats\n",
        "        })\n",
        "\n",
        "\n",
        "    stats = calculate_means(comparison_stats)\n",
        "    return stats, comparison_stats, comparison_stats_boxes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tmtyLaEPMYq"
      },
      "outputs": [],
      "source": [
        "def plot_stats_vs_num_points(comparison_stats_boxes):\n",
        "    # Filter and prepare data\n",
        "    num_points = []\n",
        "    poly_vs_abs = []\n",
        "    model_vs_abs = []\n",
        "    diff_transformed = []\n",
        "    poly_vs_abs_per_point = []\n",
        "    model_vs_abs_per_point = []\n",
        "    diff_transformed_per_point = []\n",
        "\n",
        "    for box in comparison_stats_boxes:\n",
        "        if box['num_points'] is not None and box['num_points'] > 0:\n",
        "            num_points.append(box['num_points'])\n",
        "            poly_vs_abs.append(box['poly_vs_abs'])\n",
        "            model_vs_abs.append(box['model_vs_abs'])\n",
        "            diff_transformed.append(box['diff_transformed'])\n",
        "            poly_vs_abs_per_point.append(box['poly_vs_abs_per_point'])\n",
        "            model_vs_abs_per_point.append(box['model_vs_abs_per_point'])\n",
        "            diff_transformed_per_point.append(box['diff_transformed_per_point'])\n",
        "\n",
        "    # Convert to numpy arrays for plotting\n",
        "    num_points = np.array(num_points)\n",
        "    poly_vs_abs = np.array(poly_vs_abs)\n",
        "    model_vs_abs = np.array(model_vs_abs)\n",
        "    diff_transformed = np.array(diff_transformed)\n",
        "    poly_vs_abs_per_point = np.array(poly_vs_abs_per_point)\n",
        "    model_vs_abs_per_point = np.array(model_vs_abs_per_point)\n",
        "    diff_transformed_per_point = np.array(diff_transformed_per_point)\n",
        "\n",
        "    # Create figure and axes\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    # Plot each metric vs num_points\n",
        "    axs[0].scatter( poly_vs_abs,num_points, alpha=0.6, label='poly_vs_abs')\n",
        "    axs[0].set_title('poly_vs_abs vs num_points')\n",
        "    axs[0].set_ylabel('num_points')\n",
        "    axs[0].set_xlabel('poly_vs_abs (m)')\n",
        "\n",
        "    axs[1].scatter( model_vs_abs,num_points, alpha=0.6, label='model_vs_abs', color='orange')\n",
        "    axs[1].set_title('model_vs_abs vs num_points')\n",
        "    axs[1].set_ylabel('num_points')\n",
        "    axs[1].set_xlabel('model_vs_abs (m)')\n",
        "\n",
        "    axs[2].scatter( diff_transformed,num_points, alpha=0.6, label='diff_transformed', color='green')\n",
        "    axs[2].set_title('diff_transformed vs num_points')\n",
        "    axs[2].set_ylabel('num_points')\n",
        "    axs[2].set_xlabel('diff_transformed (m)')\n",
        "\n",
        "    axs[3].scatter( poly_vs_abs_per_point,num_points, alpha=0.6, label='poly_vs_abs_per_point', color='red')\n",
        "    axs[3].set_title('poly_vs_abs_per_point vs num_points')\n",
        "    axs[3].set_ylabel('num_points')\n",
        "    axs[3].set_xlabel('poly_vs_abs_per_point (m)')\n",
        "\n",
        "    axs[4].scatter( model_vs_abs_per_point,num_points, alpha=0.6, label='model_vs_abs_per_point', color='purple')\n",
        "    axs[4].set_title('model_vs_abs_per_point vs num_points')\n",
        "    axs[4].set_ylabel('num_points')\n",
        "    axs[4].set_xlabel('model_vs_abs_per_point (m)')\n",
        "\n",
        "    axs[5].scatter( diff_transformed_per_point,num_points, alpha=0.6, label='diff_transformed_per_point', color='brown')\n",
        "    axs[5].set_title('diff_transformed_per_point vs num_points')\n",
        "    axs[5].set_ylabel('num_points')\n",
        "    axs[5].set_xlabel('diff_transformed_per_point (m)')\n",
        "\n",
        "    for ax in axs:\n",
        "        ax.grid(True)\n",
        "        ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRyvDbXO8HlA"
      },
      "outputs": [],
      "source": [
        "def test_yolo_in_batches(cam_images, depths_rel, points_dep_abs, depths_asb,\n",
        "                              model_param, poly_param, model, device='cpu'):\n",
        "    # 1. Inizializzazione modello\n",
        "    transform = ToTensor()\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "\n",
        "    # 2. Loop su ogni immagine singola\n",
        "    for img_idx in range(len(cam_images)):\n",
        "\n",
        "\n",
        "        # 4. Estrazione dati singola immagine\n",
        "        single_cam_img = cam_images[img_idx]\n",
        "        single_depth_rel = depths_rel[img_idx].detach().cpu().numpy()\n",
        "        single_points = points_dep_abs[img_idx]\n",
        "        single_depths = depths_asb[img_idx]\n",
        "        model_params = model_param[img_idx].detach().cpu().numpy()\n",
        "        poly_params = poly_param[img_idx]\n",
        "\n",
        "        # 5. Inference\n",
        "        with torch.inference_mode():\n",
        "            results = model(single_cam_img)\n",
        "\n",
        "        input_boxs = []\n",
        "        for result in results[0]:\n",
        "          boxes_tensor = result.boxes.xyxy\n",
        "\n",
        "          # Converti in NumPy array\n",
        "          boxes_np = boxes_tensor.cpu().detach().numpy()\n",
        "\n",
        "          # Se ti serve solo il primo box, ad esempio:\n",
        "          input_box = boxes_np[0]\n",
        "\n",
        "          input_boxs.append(input_box)\n",
        "\n",
        "        # 7. Estrazione parametri\n",
        "        a_model = model_params[0]\n",
        "        b_model = model_params[1]\n",
        "        c_model = model_params[2]\n",
        "\n",
        "        a_poly = poly_params[0]\n",
        "        b_poly = poly_params[1]\n",
        "        c_poly = poly_params[2]\n",
        "\n",
        "\n",
        "        # 9. Estrazione dati\n",
        "        data_model = extract_box_depth_data(\n",
        "            depth_rel=single_depth_rel,\n",
        "            input_boxes=input_boxs,\n",
        "            a=a_model,\n",
        "            b=b_model,\n",
        "            c=c_model,\n",
        "            points_dep_abs=single_points,\n",
        "            depths_abs=single_depths\n",
        "        )\n",
        "        print(data_model)\n",
        "\n",
        "        data_poly = extract_box_depth_data(\n",
        "            depth_rel=single_depth_rel,\n",
        "            input_boxes=input_boxs,\n",
        "            a=a_poly,\n",
        "            b=b_poly,\n",
        "            c=c_poly,\n",
        "            points_dep_abs=single_points,\n",
        "            depths_abs=single_depths\n",
        "        )\n",
        "        print(data_poly)\n",
        "\n",
        "        # 10. Salvataggio risultati\n",
        "        outputs.append({\n",
        "            'image_index': img_idx,\n",
        "            'model_data': data_model,\n",
        "            'poly_data': data_poly\n",
        "        })\n",
        "\n",
        "        # 11. Pulizia memoria\n",
        "        del single_cam_img, results\n",
        "        if device != 'cpu':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF1SL1_T8gza"
      },
      "outputs": [],
      "source": [
        "def test_in_batches(cam_images, depths_rel, points_dep_abs, depths_asb, model, batch_size=64, device='cpu'):\n",
        "    transform = ToTensor()\n",
        "    model.to(device)\n",
        "    a_b_c = None\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    for i in range(0, len(cam_images), batch_size):\n",
        "        # Estrai il batch\n",
        "        batch_rgb_images = cam_images[i:i+batch_size]\n",
        "        batch_depths_rel = depths_rel[i:i+batch_size]\n",
        "        batch_points_dep_abs = points_dep_abs[i:i+batch_size]\n",
        "        batch_depths_asb = depths_asb[i:i+batch_size]\n",
        "\n",
        "        # Preprocessing immagini RGB\n",
        "        rgb_tensors = [transform(img).to(device) for img in batch_rgb_images]\n",
        "        rgb_batch = torch.stack(rgb_tensors, dim=0)  # (B,3,H,W)\n",
        "\n",
        "        # Preprocessing depth relative\n",
        "        d_rel_tensors = [\n",
        "            depth_rel.clone().detach().float().to(device) if torch.is_tensor(depth_rel)\n",
        "            else torch.tensor(depth_rel).float().to(device)\n",
        "            for depth_rel in batch_depths_rel\n",
        "        ]\n",
        "\n",
        "        #d_rel_tensors = [torch.tensor(depth_rel).float().to(device) for depth_rel in batch_depths_rel]\n",
        "        d_rel_batch = torch.stack(d_rel_tensors, dim=0).unsqueeze(1)  # (B,1,H,W)\n",
        "\n",
        "        # Concatenazione\n",
        "        input_tensor = torch.cat([rgb_batch, d_rel_batch], dim=1)\n",
        "\n",
        "        # Training step\n",
        "\n",
        "        with torch.inference_mode():\n",
        "          a_b_c_hat = model(input_tensor)\n",
        "          loss = affine_depth_loss(a_b_c_hat, batch_depths_rel, batch_points_dep_abs, batch_depths_asb)\n",
        "          if a_b_c is None:\n",
        "              a_b_c = a_b_c_hat\n",
        "          else:\n",
        "              a_b_c = torch.cat((a_b_c, a_b_c_hat), dim=0)\n",
        "\n",
        "          running_loss += loss.item() * len(batch_rgb_images)\n",
        "\n",
        "\n",
        "\n",
        "        # Libera la memoria GPU se necessario\n",
        "        if device != 'cpu':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    epoch_loss = running_loss / len(cam_images)\n",
        "    print(f\"Test Loss: {epoch_loss:.4f}\")\n",
        "    return a_b_c,epoch_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlRvecqg9vVS"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def sample_30_percent_same_indices(lists):\n",
        "    total_len = len(lists[0])\n",
        "    sample_size = int(total_len * 0.3)\n",
        "    sampled_indices = sorted(random.sample(range(total_len), sample_size))\n",
        "    sampled_lists = [[lst[i] for i in sampled_indices] for lst in lists]\n",
        "    return sampled_lists, sampled_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT66wzFp9d53"
      },
      "source": [
        "## Model definition\n",
        "The model takes a 4-channel input (such as RGB plus depth from DepthAnything) and predicts three affine parameters for depth transformation. It combines multi-scale feature extraction, channel attention (SEBlock), and efficient depthwise separable convolutions to capture both local and global information, outputting the parameters through fully connected layers after global pooling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hGvUp3x9ggw"
      },
      "outputs": [],
      "source": [
        "class LightSEBlock(nn.Module):\n",
        "    def __init__(self, channel, reduction=8):  # Ridotto reduction\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "class EfficientDepthwiseConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size,\n",
        "                                 padding='same', groups=in_channels)\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)  # Un solo BN\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return F.relu(self.bn(x))\n",
        "\n",
        "class SimplifiedMultiScale(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.branch1 = nn.Conv2d(in_channels, out_channels//2, 1)\n",
        "        self.branch2 = nn.Conv2d(in_channels, out_channels//2, 3, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b1 = self.branch1(x)\n",
        "        b2 = self.branch2(x)\n",
        "        return F.relu(self.bn(torch.cat([b1, b2], dim=1)))\n",
        "\n",
        "class ScaleBiasHead(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Fase iniziale più compatta\n",
        "        self.conv1 = nn.Conv2d(4, 8, 3, padding=1)  # Canali ridotti\n",
        "        self.msff = SimplifiedMultiScale(8, 16)     # Output ridotto\n",
        "        self.se = LightSEBlock(16)\n",
        "        self.conv3 = EfficientDepthwiseConv(16, 32) # Canali ottimizzati\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(32, 3),  # Rimossa layer intermedio\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.msff(x)\n",
        "        x = self.se(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.conv3(x)\n",
        "        x = self.pool(x).flatten(1)\n",
        "        return self.fc(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYwLqILa6xbk"
      },
      "source": [
        "## Set up\n",
        "Sets up the computation device (GPU if available), loads a pre-trained depth estimation model and its image processor, assigns them to the device, and initializes the TruckScenes mini dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496,
          "referenced_widgets": [
            "a4ed3b5a744b412d800072bab9086e4e",
            "11e197800d194a9dac3e9bbce26161e8",
            "bff1630a5f4944b2937c2b96bf1cc387",
            "5ffabc59a6bd43b2aab04831ec067fdf",
            "b29a926622274da598485147c76abf3b",
            "e97ce33b0ebd47098b4da105d39a33b3",
            "e95632974ed94c6c88e4e982a37453e7",
            "a1798fca83ed4d7b93d14fb265969516",
            "6b7b0bafa91c4e7cb1bd946148a1ef39",
            "9215ca3450d94cd5bbc69257c1c2ba77",
            "98c5c5f57ef84a2b952b3b15ab4c69c4",
            "fbad99b875da43039c268f2cc4bd7229",
            "f3036fc7113542f6a6ec50b6133868a2",
            "c20d3aa7ea9a46e1bbf1ca6a46fbd4e8",
            "a5694f4a15714eaab37888356fb3f140",
            "938ff96efcc043d7bb980721a54fe5af",
            "1d07db9a91fb497c993d3001c16284dd",
            "55ffbaf581bd42b7826f1f456b926cd9",
            "e149fb09fe3844af8b8d9f93663fad8e",
            "cbdada9f5d3b4fc890fa49127d2f936c",
            "c6ef629fe0a8444997628d9e23f8b869",
            "7d021abe4e19484485be145df6d7f825",
            "bc84acfda41e45728ab41abbe2d2e836",
            "b422c6253c6e44329998c158d6829e96",
            "9c37b82accb846bb8c530cc5a15e8a78",
            "d4eb2f2adeaf4685aa703e964d2872ab",
            "58814fb49e6547a981299c0599201800",
            "621cebb99aa54a5797bd0fcc206a14c5",
            "d206236ac02445edb7f481e1cba4c011",
            "abb3d148fb3b4d828caa0c91c6349839",
            "573bced22f064b41b2bacf532139e89e",
            "2640ef34b8f04a54a11cfcc3e476a589",
            "d514c79fe11349b0bea520ef8dd43b41"
          ]
        },
        "id": "-dr5pIHwpC7G",
        "outputId": "a2ec9521-ff61-496d-c9fd-fcd11e700bdc"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"depth-anything/Depth-Anything-V2-Small-hf\",use_fast=True)\n",
        "image_processor.device = device\n",
        "model = AutoModelForDepthEstimation.from_pretrained(\"depth-anything/Depth-Anything-V2-Small-hf\")\n",
        "model.to(device)\n",
        "trucksc = TruckScenes('v1.0-mini', '/content/man-truckscenes', True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkZ3z9riMWWn"
      },
      "source": [
        "### Balance Objectives\n",
        "\n",
        "- **Categories balanced:** weather, area, daytime, season, lighting, structure, construction.\n",
        "- **Train set:** Includes diverse conditions (e.g., clear/overcast/rain, city/highway/terminal, various seasons and lighting).\n",
        "- **Test set:** Also covers all key conditions, including rare cases (snow, roadworks, overpass, twilight, other lighting).\n",
        "\n",
        "**Motivation:**  \n",
        "Both train and test sets are constructed to ensure representation of all important scenario combinations, including rare weather, lighting, and structural conditions, supporting robust and fair model evaluation.\n",
        "\n",
        "\n",
        "#### Train set\n",
        "- **Scene 0:** clear, terminal, noon, autumn, illuminated, regular, unchanged\n",
        "- **Scene 2:** overcast, city, noon, winter, illuminated, bridge, unchanged\n",
        "- **Scene 3:** overcast, highway, morning, summer, illuminated, regular, unchanged\n",
        "- **Scene 6:** clear, city, noon, winter, twilight, regular, unchanged\n",
        "- **Scene 8:** rain, highway, noon, autumn, illuminated, regular, unchanged\n",
        "\n",
        "#### Test set\n",
        "- **Scene 1:** clear, highway, noon, autumn, dark, overpass, unchanged\n",
        "- **Scene 4:** overcast, highway, morning, summer, illuminated, regular, roadworks\n",
        "- **Scene 5:** overcast, terminal, morning, summer, illuminated, regular, unchanged\n",
        "- **Scene 7:** snow, city, noon, autumn, illuminated, regular, unchanged\n",
        "- **Scene 9:** clear, city, noon, winter, other_lighting, regular, unchanged\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09Clh_m6_rVA"
      },
      "source": [
        "## Visualizes the LIDAR points\n",
        "Visualizes the LIDAR point cloud projected onto the left front camera image. It will be the starting point of our experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "X5lhJJuIZ4h9",
        "outputId": "e1d5172f-e8d1-425f-a925-deca68c09b1c"
      },
      "outputs": [],
      "source": [
        "my_sample = trucksc.sample[3]\n",
        "print(my_sample)\n",
        "sensor = 'CAMERA_LEFT_FRONT'\n",
        "cam_front_data = trucksc.get('sample_data', my_sample['data'][sensor])\n",
        "print(cam_front_data['filename'])\n",
        "trucksc.render_pointcloud_in_image(my_sample['token'], pointsensor_channel='LIDAR_LEFT', camera_channel='CAMERA_LEFT_FRONT', dot_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "_zT6VuwMZ-Kx",
        "outputId": "74e063f7-ebce-4abd-b646-674d4f06e92f"
      },
      "outputs": [],
      "source": [
        "trucksc.render_pointcloud_in_image(my_sample['token'], pointsensor_channel='LIDAR_RIGHT', camera_channel='CAMERA_LEFT_FRONT', dot_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sWEFXROhiMN"
      },
      "outputs": [],
      "source": [
        "del my_sample, sensor, cam_front_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGorIpXgAl5f"
      },
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "In this phase, we will begin preparing our train_set and test_set by extracting images, point cloud masks, depths and descriptions of the scens from the available sensors and cameras in the dataset. For computational efficiency, we will reduce the size of the test_set to one third of its original size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGd9EN0rWjPj"
      },
      "outputs": [],
      "source": [
        "numberScene = [0,2,3,6,8]\n",
        "numberSceneTest = [1,4,5,7,9]\n",
        "sensors = ['CAMERA_LEFT_FRONT','CAMERA_RIGHT_FRONT','CAMERA_LEFT_BACK','CAMERA_RIGHT_BACK']\n",
        "sensors_lidar = ['LIDAR_LEFT','LIDAR_RIGHT','LIDAR_LEFT','LIDAR_RIGHT']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "517cbeLpah9Y"
      },
      "outputs": [],
      "source": [
        "cam_images = []\n",
        "depths_asb = []\n",
        "points_dep_abs = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpboeNmGo7Oh"
      },
      "outputs": [],
      "source": [
        "for n in numberScene:\n",
        "  my_scene = trucksc.scene[n]\n",
        "  first_sample_token = my_scene['first_sample_token']\n",
        "  my_sample = trucksc.get('sample', first_sample_token)\n",
        "\n",
        "  while my_sample['next'] is not None and my_sample['next'] != '':\n",
        "    for sensor,lidar in zip(sensors,sensors_lidar):\n",
        "      cam_data = trucksc.get('sample_data', my_sample['data'][sensor])\n",
        "      image = Image.open('/content/man-truckscenes/'+cam_data['filename'])\n",
        "      cam_images.append(image)\n",
        "      points, _, _, depths = render_pointcloud_in_deapth(my_sample['token'], pointsensor_channel=lidar, camera_channel=sensor)\n",
        "      depths_asb.append(depths)\n",
        "      points_dep_abs.append(points)\n",
        "    my_sample = trucksc.get('sample', my_sample['next'])\n",
        "\n",
        "del cam_data, image, points, depths,first_sample_token, my_sample, my_scene,sensor,lidar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT5MgJKIy3Gu",
        "outputId": "00765242-baec-403d-9461-b0bb7f356975"
      },
      "outputs": [],
      "source": [
        "print(len(cam_images))\n",
        "print(len(depths_asb))\n",
        "print(len(points_dep_abs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "235lg6TpKmYU"
      },
      "outputs": [],
      "source": [
        "cam_images_test = []\n",
        "depths_asb_test = []\n",
        "points_dep_abs_test = []\n",
        "desc_test = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBpUOuTEKlo7"
      },
      "outputs": [],
      "source": [
        "for n in numberSceneTest:\n",
        "  my_scene = trucksc.scene[n]\n",
        "  first_sample_token = my_scene['first_sample_token']\n",
        "  my_sample = trucksc.get('sample', first_sample_token)\n",
        "\n",
        "  while my_sample['next'] is not None and my_sample['next'] != '':\n",
        "    for sensor,lidar in zip(sensors,sensors_lidar):\n",
        "      cam_data = trucksc.get('sample_data', my_sample['data'][sensor])\n",
        "      image = Image.open('/content/man-truckscenes/'+cam_data['filename'])\n",
        "      cam_images_test.append(image)\n",
        "      desc_test.append(my_scene['description'])\n",
        "      points, _, _, depths = render_pointcloud_in_deapth(my_sample['token'], pointsensor_channel=lidar, camera_channel=sensor)\n",
        "      depths_asb_test.append(depths)\n",
        "      points_dep_abs_test.append(points)\n",
        "    my_sample = trucksc.get('sample', my_sample['next'])\n",
        "del cam_data, image, points, depths,first_sample_token, my_sample, my_scene,sensor,lidar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaRiv1g1xzvo",
        "outputId": "f11f8af5-edda-4fed-c2c1-63f0f94afffc"
      },
      "outputs": [],
      "source": [
        "sampled_lists, sampled_indices = sample_30_percent_same_indices([cam_images_test, depths_asb_test, points_dep_abs_test,desc_test])\n",
        "sampled_cam_images_test, sampled_depths_asb_test, sampled_points_dep_abs_test , sampled_desc_test = sampled_lists\n",
        "\n",
        "print(\"Sampled cam_images_test length:\", len(sampled_cam_images_test))\n",
        "print(\"Sampled depths_asb_test length:\", len(sampled_depths_asb_test))\n",
        "print(\"Sampled points_dep_abs_test length:\", len(sampled_points_dep_abs_test))\n",
        "print(\"Sampled desc_test length:\", len(sampled_desc_test))\n",
        "\n",
        "cam_images_test = sampled_cam_images_test\n",
        "depths_asb_test = sampled_depths_asb_test\n",
        "points_dep_abs_test = sampled_points_dep_abs_test\n",
        "desc_test = sampled_desc_test\n",
        "del sampled_cam_images_test, sampled_depths_asb_test, sampled_points_dep_abs_test, sampled_lists, sampled_indices, sampled_desc_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iig5AhCFBEFd"
      },
      "source": [
        "## Generating Relative Depth Heatmaps\n",
        "\n",
        "The next step is to input both the train and test sets into DepthAnything2 in order to obtain the corresponding relative depth heatmaps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLm8meZepjoO"
      },
      "outputs": [],
      "source": [
        "depths_rel = []\n",
        "for image in cam_images:\n",
        "  inputs = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output = model(**inputs)\n",
        "  post_processed_output = image_processor.post_process_depth_estimation(\n",
        "      output,\n",
        "      target_sizes=[(image.height, image.width)],\n",
        "  )\n",
        "  depths_rel.append(post_processed_output[0][\"predicted_depth\"])\n",
        "  # Libera memoria GPU non più necessaria\n",
        "  del inputs, output, post_processed_output\n",
        "  torch.cuda.empty_cache()  # Solo se usi la GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-wPl5-Wqeqn"
      },
      "outputs": [],
      "source": [
        "# prepare image for the model\n",
        "depths_rel_test = []\n",
        "\n",
        "for image in cam_images_test:\n",
        "  inputs = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output = model(**inputs)\n",
        "  post_processed_output = image_processor.post_process_depth_estimation(\n",
        "      output,\n",
        "      target_sizes=[(image.height, image.width)],\n",
        "  )\n",
        "  depths_rel_test.append(post_processed_output[0][\"predicted_depth\"])\n",
        "  # Libera memoria GPU non più necessaria\n",
        "  del inputs, output, post_processed_output\n",
        "  torch.cuda.empty_cache()  # Solo se usi la GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4pe2PcKaNd5"
      },
      "outputs": [],
      "source": [
        "del model, image_processor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-JNkPPxB1cO"
      },
      "source": [
        "### Parameter Extraction via Polynomial Regression  \n",
        "This part calculates **linear (1st-degree)** and **quadratic (2nd-degree)** polynomial coefficients (`a`, `b` or `a`, `b`, `c`) to model the relationship between relative depth values and absolute depth measurements.\n",
        "This operation is done for train_Set e test_Set.\n",
        "\n",
        "**Key Operations:**  \n",
        "1. **Data Alignment**:  \n",
        "   - Extracts valid (x,y) coordinates from LiDAR point clouds projected onto the depth map.  \n",
        "   - Inverts relative depth values (`depth_rel = max_val - depth_rel_img`) to align with absolute depth conventions.  \n",
        "\n",
        "2. **Filtering**:  \n",
        "   - Applies masks to exclude invalid (non-finite or ≤0) depth values.  \n",
        "\n",
        "3. **Regression**:  \n",
        "   - Uses `np.polyfit` to compute optimal coefficients for:  \n",
        "     - **Linear**: `D_abs ≈ a·D_rel + b`  \n",
        "     - **Quadratic**: `D_abs ≈ a·D_rel² + b·D_rel + c`  \n",
        "\n",
        "**Output**:  \n",
        "- Stores coefficients in `params_depth` for later use in depth transformation tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pxD3naqG-e7"
      },
      "outputs": [],
      "source": [
        "params_depth = []\n",
        "for depth_rel_img, points, depth_abs in zip(depths_rel, points_dep_abs, depths_asb):\n",
        "    xs = points[0].astype(int)\n",
        "    ys = points[1].astype(int)\n",
        "\n",
        "    depth_rel_inv = depth_rel_img\n",
        "    depth_rel = depth_rel_inv.max() - depth_rel_inv\n",
        "\n",
        "    depth_rel_pts = depth_rel[ys, xs]\n",
        "    Dabs_tensor = torch.from_numpy(depth_abs).float().to(depth_rel_pts.device)\n",
        "\n",
        "\n",
        "    mask = (depth_rel_pts > 0) & torch.isfinite(depth_rel_pts)\n",
        "    Drel = depth_rel_pts[mask]\n",
        "    Dabs = Dabs_tensor[mask]\n",
        "\n",
        "\n",
        "    a, b = np.polyfit(Drel.cpu().numpy(), Dabs.cpu().numpy(), 1)\n",
        "    params_depth.append((a, b))\n",
        "del depth_rel_img, points, depth_abs, xs, ys, depth_rel_inv, depth_rel, depth_rel_pts, Dabs_tensor, mask, Drel, Dabs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkaeQXHzMDA1"
      },
      "outputs": [],
      "source": [
        "params_depth_train_3d = []\n",
        "for depth_rel_img, points, depth_abs in zip(depths_rel, points_dep_abs, depths_asb):\n",
        "    xs = points[0].astype(int)\n",
        "    ys = points[1].astype(int)\n",
        "\n",
        "    depth_rel_inv = depth_rel_img\n",
        "    depth_rel = depth_rel_inv.max() - depth_rel_inv\n",
        "\n",
        "    depth_rel_pts = depth_rel[ys, xs]\n",
        "    Dabs_tensor = torch.from_numpy(depth_abs).float().to(depth_rel_pts.device)\n",
        "\n",
        "\n",
        "    mask = (depth_rel_pts > 0) & torch.isfinite(depth_rel_pts)\n",
        "    Drel = depth_rel_pts[mask]\n",
        "    Dabs = Dabs_tensor[mask]\n",
        "\n",
        "\n",
        "    a, b, c = np.polyfit(Drel.cpu().numpy(), Dabs.cpu().numpy(), 2)\n",
        "    params_depth_train_3d.append((a, b,c))\n",
        "del depth_rel_img, points, depth_abs, xs, ys, depth_rel_inv, depth_rel, depth_rel_pts, Dabs_tensor, mask, Drel, Dabs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mTT_o9JoKME"
      },
      "outputs": [],
      "source": [
        "params_depth_test = []\n",
        "for depth_rel_img, points, depth_abs in zip(depths_rel_test, points_dep_abs_test, depths_asb_test):\n",
        "    xs = points[0].astype(int)\n",
        "    ys = points[1].astype(int)\n",
        "\n",
        "    depth_rel_inv = depth_rel_img\n",
        "    depth_rel = depth_rel_inv.max() - depth_rel_inv\n",
        "\n",
        "    depth_rel_pts = depth_rel[ys, xs]\n",
        "    Dabs_tensor = torch.from_numpy(depth_abs).float().to(depth_rel_pts.device)\n",
        "\n",
        "\n",
        "    mask = (depth_rel_pts > 0) & torch.isfinite(depth_rel_pts)\n",
        "    Drel = depth_rel_pts[mask]\n",
        "    Dabs = Dabs_tensor[mask]\n",
        "\n",
        "\n",
        "    a, b = np.polyfit(Drel.cpu().numpy(), Dabs.cpu().numpy(), 1)\n",
        "    params_depth_test.append((a, b))\n",
        "del depth_rel_img, points, depth_abs, xs, ys, depth_rel_inv, depth_rel, depth_rel_pts, Dabs_tensor, mask, Drel, Dabs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lE5XGR_dpMDD"
      },
      "outputs": [],
      "source": [
        "params_depth_test_3d = []\n",
        "for depth_rel_img, points, depth_abs in zip(depths_rel_test, points_dep_abs_test, depths_asb_test):\n",
        "    xs = points[0].astype(int)\n",
        "    ys = points[1].astype(int)\n",
        "\n",
        "    depth_rel_inv = depth_rel_img\n",
        "    depth_rel = depth_rel_inv.max() - depth_rel_inv\n",
        "\n",
        "    depth_rel_pts = depth_rel[ys, xs]\n",
        "    Dabs_tensor = torch.from_numpy(depth_abs).float().to(depth_rel_pts.device)\n",
        "\n",
        "\n",
        "    mask = (depth_rel_pts > 0) & torch.isfinite(depth_rel_pts)\n",
        "    Drel = depth_rel_pts[mask]\n",
        "    Dabs = Dabs_tensor[mask]\n",
        "\n",
        "\n",
        "    a, b, c = np.polyfit(Drel.cpu().numpy(), Dabs.cpu().numpy(), 2)\n",
        "    params_depth_test_3d.append((a, b,c))\n",
        "del depth_rel_img, points, depth_abs, xs, ys, depth_rel_inv, depth_rel, depth_rel_pts, Dabs_tensor, mask, Drel, Dabs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0hXGrXTCXBe"
      },
      "source": [
        "## Model Training Setup and Execution\n",
        "\n",
        "### **Implementation Steps**  \n",
        "#### 1. **Hardware Configuration**  \n",
        "   - Automatically selects GPU (`cuda`) if available, otherwise uses CPU\n",
        "\n",
        "#### 2. **Model Initialization**  \n",
        "   - Instantiates `ScaleBiasHead` (multi-scale fusion + SEBlocks)  \n",
        "   - Moves model to selected device (GPU/CPU)\n",
        "\n",
        "#### 3. **Optimizer Configuration**  \n",
        "   - Adam optimizer with learning rate: $\\alpha = 0.001$\n",
        "\n",
        "#### 4. **Training Process** (`train_in_batches`)  \n",
        "   - **Input Data Batches**:  \n",
        "     - RGB images (`cam_images`)  \n",
        "     - Relative depth maps (`depths_rel`)  \n",
        "     - LiDAR projections (`points_dep_abs`, `depths_asb`)  \n",
        "     - Polynomial coefficients (`params_depth_train_3d`)\n",
        "   - **Batch Processing**:  \n",
        "     - Batch size = 4 (memory-constrained)  \n",
        "     - Input: Concatenated RGB + depth channels  \n",
        "     - Training duration: 100 epochs\n",
        "  \n",
        "\n",
        "### 5. Hybrid Loss Components\n",
        "\n",
        "**Depth Loss (MSE):**\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{depth}} = \\frac{1}{N}\\sum_{i=1}^N \\left(D_{\\text{pred}}^{(i)} - D_{\\text{abs}}^{(i)}\\right)^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $D_{\\text{pred}}^{(i)}$ = Model-predicted depth  \n",
        "- $D_{\\text{abs}}^{(i)}$ = LiDAR-measured depth  \n",
        "- $N$ = Number of valid LiDAR points  \n",
        "\n",
        "**Parameter Loss (Huber):**\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{param}} = \\frac{1}{B}\\sum_{j=1}^B \\sum_{k\\in\\{a,b,c\\}}\n",
        "\\begin{cases}\n",
        "0.5\\Delta^2 & \\text{if } |\\hat{k}_j - k_j| \\leq \\Delta \\\\\n",
        "\\Delta(|\\hat{k}_j - k_j| - 0.5\\Delta) & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $\\hat{k}_j$ = Predicted parameters  \n",
        "- $k_j$ = Polynomial parameters  \n",
        "- $\\Delta = 1.0$  \n",
        "\n",
        "**Total Loss:**\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{depth}} + 0.5\\mathcal{L}_{\\text{param}}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Purpose**  \n",
        "Trains model to predict optimal affine parameters $(a,b,c)$ that:  \n",
        "1. Minimize depth prediction error vs LiDAR measurements  \n",
        "2. Maintain physical plausibility through polynomial constraints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ub3tS5qgAQD",
        "outputId": "21289aec-ce84-460a-ae75-ae97457401d8"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_Scale = ScaleBiasHead().to(device)\n",
        "optimizer = optim.AdamW(model_Scale.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "train_in_batches(\n",
        "    cam_images, depths_rel, points_dep_abs, depths_asb,params_depth_train_3d,\n",
        "    cam_images_test, depths_rel_test, points_dep_abs_test, depths_asb_test,\n",
        "    model_Scale, optimizer, num_epochs=70, batch_size=8, device=device,patience=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5ssHiX3xCDd"
      },
      "outputs": [],
      "source": [
        "del cam_images, depths_asb, points_dep_abs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBFQb5TDDO_R"
      },
      "source": [
        "## Evaluation\n",
        "This section of the notebook evaluates and compares the performance of different depth transformation approaches on the test set.\n",
        "\n",
        "- The function `test_in_batches` is used to generate the predicted affine parameters (a, b, c) for each test sample using the trained model. The average test loss for the model’s predictions is printed after one epoch, indicating the model’s overall error in transforming relative to absolute depth values.\n",
        "\n",
        "- Next, two baseline comparisons are performed using polynomial regression:\n",
        "  - For the linear case (2D Polyfit), the parameters obtained from fitting a first-degree polynomial to the data are converted to tensors and evaluated using the `affine_depth_loss_2d` function. The resulting loss quantifies how well a simple linear transformation fits the test data.\n",
        "  - For the quadratic case (3D Polyfit), the second-degree polynomial parameters are similarly tested with the `affine_depth_loss` function, and the corresponding loss is printed.\n",
        "\n",
        "- By reporting the test losses for the model, the 2D polyfit, and the 3D polyfit, this part enables a direct comparison between the learned model’s predictions and the best possible polynomial fits, highlighting the accuracy and potential advantages of the model over traditional regression approaches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytfWsmkyrj8L",
        "outputId": "e966e6ec-8bcd-4a7c-c8c2-aad115879858"
      },
      "outputs": [],
      "source": [
        "a_b_c_test,_ = test_in_batches(\n",
        "    cam_images_test, depths_rel_test, points_dep_abs_test, depths_asb_test,\n",
        "    model_Scale, batch_size=1, device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5bBZzZ52rr6",
        "outputId": "60261770-7e8d-48ec-9be3-f76a492534a9"
      },
      "outputs": [],
      "source": [
        "target_a_b_tensor = torch.tensor(params_depth_test, dtype=torch.float32).to(device)\n",
        "loss = affine_depth_loss_2d(target_a_b_tensor, depths_rel_test, points_dep_abs_test,depths_asb_test)\n",
        "print(f\"Test Loss with 2D Polyfit : {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cobDivdn2HNt",
        "outputId": "732f2bdf-6db2-44f9-e391-b3471a3009f2"
      },
      "outputs": [],
      "source": [
        "target_a_b_c_tensor = torch.tensor(params_depth_test_3d, dtype=torch.float32).to(device)\n",
        "loss = affine_depth_loss(target_a_b_c_tensor, depths_rel_test, points_dep_abs_test,depths_asb_test)\n",
        "print(f\"Test Loss with 3D Polyfit : {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU9eCMpDDs5F"
      },
      "source": [
        "## Evaluation in different condition\n",
        "This section of the notebook evaluates model performance under specific lighting conditions—here, for the \"illuminated\" subset, but the same approach is repeated for \"dark\" and \"other_lighting\" conditions.\n",
        "\n",
        "- The dataset is first filtered to select only the samples with the desired lighting condition (e.g., \"illuminated\").\n",
        "- The number of filtered samples is checked, and if present, the evaluation proceeds.\n",
        "- The trained model predicts affine depth transformation parameters on the filtered test set using `test_in_batches`, and the test loss is reported.\n",
        "- For comparison, the loss is also computed using the best-fit linear (2D polyfit) and quadratic (3D polyfit) polynomial parameters for the same data, using the respective loss functions.\n",
        "- All results are printed, allowing direct comparison of the model’s performance against the polynomial baselines for each lighting condition.\n",
        "\n",
        "This process enables a detailed analysis of how well the model and polynomial baselines perform under different lighting scenarios, highlighting strengths and weaknesses in varying real-world conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMnJ06ERGQcM"
      },
      "outputs": [],
      "source": [
        "(cam_images_ill, depths_rel_ill, desc_ill,\n",
        " points_dep_abs_ill, depths_asb_ill, params_depth_ill, params_depth_ill_3d, a_b_c_test_i) = filter_by_lighting(\n",
        "    \"illuminated\",\n",
        "    cam_images_test, depths_rel_test, desc_test,\n",
        "    points_dep_abs_test, depths_asb_test, params_depth_test, params_depth_test_3d, a_b_c_test\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2yiAFOQE8y-",
        "outputId": "f5779b55-b34e-4f74-9fe5-98cc3b53f852"
      },
      "outputs": [],
      "source": [
        "len(cam_images_ill)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khjfB_sSGdoU",
        "outputId": "0905e01e-9e91-405a-ffd3-71cd2f0d8a74"
      },
      "outputs": [],
      "source": [
        "if len(cam_images_ill) > 0:\n",
        "  print(\"Condition : Illuminated\")\n",
        "\n",
        "    # Stack the list of tensors into a single batch tensor\n",
        "  target_a_b_c_tensor_i = torch.stack(a_b_c_test_i).to(device)\n",
        "  loss = affine_depth_loss(target_a_b_c_tensor_i, depths_rel_ill, points_dep_abs_ill,depths_asb_ill)\n",
        "  print(f\"Test Loss with 3D Model : {loss.item():.4f}\")\n",
        "  target_a_b_tensor_i = torch.tensor(params_depth_ill, dtype=torch.float32).to(device)\n",
        "  loss = affine_depth_loss_2d(target_a_b_tensor_i, depths_rel_ill, points_dep_abs_ill,depths_asb_ill)\n",
        "  print(f\"Test Loss with 2D Polyfit : {loss.item():.4f}\")\n",
        "  target_a_b_c_tensor_i = torch.tensor(params_depth_ill_3d, dtype=torch.float32).to(device)\n",
        "  loss = affine_depth_loss(target_a_b_c_tensor_i, depths_rel_ill, points_dep_abs_ill,depths_asb_ill)\n",
        "  print(f\"Test Loss with 3D Polyfit : {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvwSppfIGSoG"
      },
      "outputs": [],
      "source": [
        "(cam_images_d, depths_rel_d, desc_d,\n",
        " points_dep_abs_d, depths_asb_d, params_depth_d, params_depth_d_3d, a_b_c_test_d) = filter_by_lighting(\n",
        "    \"dark\",\n",
        "    cam_images_test, depths_rel_test, desc_test,\n",
        "    points_dep_abs_test, depths_asb_test, params_depth_test, params_depth_test_3d, a_b_c_test\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DwMWDOTEnTq",
        "outputId": "9154b240-5aee-4ce2-e0c8-d719205bd280"
      },
      "outputs": [],
      "source": [
        "len(cam_images_d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f-ufwrFHHFN",
        "outputId": "ad014e72-64db-47e4-d1f2-1b8f5f5911b7"
      },
      "outputs": [],
      "source": [
        "if len(cam_images_d) > 0:\n",
        "  print(\"Condition : Dark\")\n",
        "\n",
        "  target_a_b_c_tensor_d = torch.stack(a_b_c_test_i).to(device)\n",
        "  loss = affine_depth_loss(target_a_b_c_tensor_d, depths_rel_ill, points_dep_abs_ill,depths_asb_ill)\n",
        "  print(f\"Test Loss with 3D Model : {loss.item():.4f}\")\n",
        "  target_a_b_tensor_d = torch.tensor(params_depth_d, dtype=torch.float32).to(device)\n",
        "  loss = affine_depth_loss_2d(target_a_b_tensor_d, depths_rel_d, points_dep_abs_d,depths_asb_d)\n",
        "  print(f\"Test Loss with 2D Polyfit : {loss.item():.4f}\")\n",
        "  target_a_b_c_tensor_d = torch.tensor(params_depth_d_3d, dtype=torch.float32).to(device)\n",
        "  loss = affine_depth_loss(target_a_b_c_tensor_d, depths_rel_d, points_dep_abs_d,depths_asb_d)\n",
        "  print(f\"Test Loss with 3D Polyfit : {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_x_N3tgGTXE"
      },
      "outputs": [],
      "source": [
        "(cam_images_o, depths_rel_o, desc_o,\n",
        " points_dep_abs_o, depths_asb_o, params_depth_o, params_depth_o_3d, a_b_c_test_o) = filter_by_lighting(\n",
        "    \"other_lighting\",\n",
        "    cam_images_test, depths_rel_test, desc_test,\n",
        "    points_dep_abs_test, depths_asb_test, params_depth_test, params_depth_test_3d, a_b_c_test\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uskpI4ifIcGb",
        "outputId": "827a36e6-4371-45d7-e2da-4ae52974d09b"
      },
      "outputs": [],
      "source": [
        "len(cam_images_o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hyr-zUhAHH0n",
        "outputId": "2ec1b237-76a9-48b7-ffb0-b23e2cb702f2"
      },
      "outputs": [],
      "source": [
        "if len(cam_images_o) > 0:\n",
        "  print(\"Condition : Other_lighting\")\n",
        "  target_a_b_c_tensor_o = torch.stack(a_b_c_test_i).to(device)\n",
        "  loss = affine_depth_loss(target_a_b_c_tensor_o, depths_rel_ill, points_dep_abs_ill,depths_asb_ill)\n",
        "  print(f\"Test Loss with 3D Model : {loss.item():.4f}\")\n",
        "  target_a_b_tensor_o = torch.tensor(params_depth_o, dtype=torch.float32).to(device)\n",
        "  loss = affine_depth_loss_2d(target_a_b_tensor_o, depths_rel_o, points_dep_abs_o,depths_asb_o)\n",
        "  print(f\"Test Loss with 2D Polyfit : {loss.item():.4f}\")\n",
        "  target_a_b_c_tensor_o = torch.tensor(params_depth_o_3d, dtype=torch.float32).to(device)\n",
        "  loss = affine_depth_loss(target_a_b_c_tensor_o, depths_rel_o, points_dep_abs_o,depths_asb_o)\n",
        "  print(f\"Test Loss with 3D Polyfit : {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPNQ15W4EMg4"
      },
      "source": [
        "## Tangible Evaluation Pipeline  \n",
        "This phase transitions from abstract loss metrics to **physically interpretable depth accuracy measures** by combining object detection (YOLO) with depth transformation analysis.  \n",
        "\n",
        "**Process Flow:**  \n",
        "1. **Bounding Box Generation** (`test_yolo_in_batches`):  \n",
        "   - YOLO detects objects and outputs bounding boxes for each test image.  \n",
        "   - For each box, two depth estimates are computed:  \n",
        "     - **Model-Predicted**: Using learned affine parameters (`a,b,c`) from `ScaleBiasHead`.  \n",
        "     - **Polynomial-Estimated**: Using precomputed quadratic/linear coefficients from `polyfit`.  \n",
        "\n",
        "2. **Error Quantification** (`extract_comparison_stats`):  \n",
        "   - For every bounding box, three metrics are calculated:  \n",
        "     - **Mean Depth Error**: Absolute difference (meters) between model/polynomial estimates and LiDAR-measured ground truth.  \n",
        "     - **Per-Point Error**: Error normalized by the number of LiDAR points in the box (accounts for sparse/dense measurements).  \n",
        "     - **Box Overlap (IoU)**: Intersection-over-Union with ground-truth boxes (if available).  \n",
        "\n",
        "**Key Metrics Reported:**  \n",
        "- model_vs_abs_stats_mean: This metric represents the average error of the model's depth predictions compared to LiDAR ground truth, measured in meters. It provides an overall indication of the model's depth accuracy.\n",
        "\n",
        "- poly_vs_abs_stats_mean: This metric shows the average error of the polynomial baseline's depth estimates compared to LiDAR measurements. It is used to compare the model's performance against traditional regression methods.\n",
        "\n",
        "- box_stats: num_points: This indicates the number of LiDAR points contained within each bounding box. It serves as a reliability indicator for the depth measurements, as more points generally mean more reliable data.\n",
        "\n",
        "- diff_transformed_per_point: This metric measures the difference in depth error per LiDAR point between the model's predictions and the polynomial baseline. It helps analyze the effectiveness of the model's affine parameter predictions compared to the polynomial approach.\n",
        "\n",
        "**Advantages Over Loss Metrics:**  \n",
        "- **Interpretability**: Errors in meters (e.g., \"model is 0.45m more accurate than polynomial on snow scenes\").  \n",
        "- **Detection-Aware**: Ties depth errors to specific objects (e.g., trucks vs signs).  \n",
        "- **Scenario Breakdown**: Enables per-condition analysis (lighting, weather) via `filter_by_lighting`.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rT7uWI9mAtm",
        "outputId": "9f12c0e4-c819-4e60-b9d8-bf0eb0d6d802"
      },
      "outputs": [],
      "source": [
        "yolo = YOLO(\"yolo11n.pt\")\n",
        "distances = test_yolo_in_batches(\n",
        "    cam_images_test, depths_rel_test, points_dep_abs_test, depths_asb_test,\n",
        "     a_b_c_test,params_depth_test_3d, yolo,device)\n",
        "distances_ill = test_yolo_in_batches(\n",
        "    cam_images_ill, depths_rel_ill, points_dep_abs_ill, depths_asb_ill,\n",
        "     a_b_c_test_i,params_depth_ill_3d, yolo,device)\n",
        "distances_d = test_yolo_in_batches(\n",
        "    cam_images_d, depths_rel_d, points_dep_abs_d, depths_asb_d,\n",
        "     a_b_c_test_d,params_depth_d_3d, yolo,device)\n",
        "distances_o = test_yolo_in_batches(\n",
        "    cam_images_o, depths_rel_o, points_dep_abs_o, depths_asb_o,\n",
        "     a_b_c_test_o,params_depth_o_3d, yolo,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtJeqfBd0FTw"
      },
      "outputs": [],
      "source": [
        "stats,comparison_stats,comparison_stats_boxes = extract_comparison_stats(distances)\n",
        "stats_ill,comparison_stats_ill,comparison_stats_ill_boxes = extract_comparison_stats(distances_ill)\n",
        "stats_d,comparison_stats_d,comparison_stats_ill_d = extract_comparison_stats(distances_d)\n",
        "stats_o,comparison_stats_o,comparison_stats_ill_o = extract_comparison_stats(distances_o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RsZpjUrRW6v",
        "outputId": "b49e2ba1-8eba-49d7-dc44-0a527dec3d2a"
      },
      "outputs": [],
      "source": [
        "print(\"Stats of all test_set: \")\n",
        "print(stats)\n",
        "print(\"Stats of Illuminated test_set: \")\n",
        "print(stats_ill)\n",
        "print(\"Stats of Dark test_set: \")\n",
        "print(stats_d)\n",
        "print(\"Stats of Other_lighting test_set: \")\n",
        "print(stats_o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "id": "pKqB_oDuPdOU",
        "outputId": "34a82318-c45e-4c97-cafe-07d7b086100c"
      },
      "outputs": [],
      "source": [
        "plot_stats_vs_num_points(comparison_stats_boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "AKlXH0gIR9nW",
        "outputId": "3222070b-8f24-4f81-a32f-f84f0ca0af08"
      },
      "outputs": [],
      "source": [
        "plot_stats_vs_num_points(comparison_stats_ill_boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "bUhrdEZDSABL",
        "outputId": "b5109a49-2f5b-4c00-ce0a-6959a4d717ce"
      },
      "outputs": [],
      "source": [
        "plot_stats_vs_num_points(comparison_stats_ill_d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "id": "zG213ZbiSBDC",
        "outputId": "0f8cdeb7-8305-44d9-8294-a32528c06555"
      },
      "outputs": [],
      "source": [
        "plot_stats_vs_num_points(comparison_stats_ill_o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Weojc3WIYkZ6",
        "outputId": "e49757f3-b7b6-4b81-bc50-3dbfe81d72ed"
      },
      "outputs": [],
      "source": [
        "if len(comparison_stats_boxes) > 0:\n",
        "  plot_comparison_curves(comparison_stats_boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s6hvHJnIYs_C",
        "outputId": "fc1d6a0b-f004-4bc2-9661-d1d81e495ef5"
      },
      "outputs": [],
      "source": [
        "if len(comparison_stats_ill_boxes) > 0:\n",
        "  plot_comparison_curves(comparison_stats_ill_boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kyf_1Pu3Yvsp",
        "outputId": "58ff17c2-89c7-4da7-9e37-84a45876056d"
      },
      "outputs": [],
      "source": [
        "if len(comparison_stats_ill_d) > 0:\n",
        "  plot_comparison_curves(comparison_stats_ill_d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uYpUTwryYx-k",
        "outputId": "9246ed4d-cc1c-4c22-e3eb-10df00aeba07"
      },
      "outputs": [],
      "source": [
        "if len(comparison_stats_ill_o) > 0:\n",
        "  plot_comparison_curves(comparison_stats_ill_o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZoLxAji21aq",
        "outputId": "8d773683-7aac-4528-a482-a06c3eb964e3"
      },
      "outputs": [],
      "source": [
        "if len(comparison_stats) > 0:\n",
        "  print(\"One element of all dataset\")\n",
        "  print_comparison_stats(comparison_stats[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtDTFx3CSIp4",
        "outputId": "de8de3a6-8947-426b-aded-00271a2e763d"
      },
      "outputs": [],
      "source": [
        "if len(comparison_stats_ill) > 0:\n",
        "  print(\"One element of Illuminated part of dataset\")\n",
        "  print_comparison_stats(comparison_stats_ill[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3TvCXBSSJni",
        "outputId": "3bca3053-b4c8-4c87-a4f0-41448b8e1120"
      },
      "outputs": [],
      "source": [
        "if len(comparison_stats_d) > 0:\n",
        "  print(\"One element of Dark part of dataset\")\n",
        "  print_comparison_stats(comparison_stats_d[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaX6H4UESKtN",
        "outputId": "f6f96dbf-34e8-49f4-8d90-1e779b0e25d5"
      },
      "outputs": [],
      "source": [
        "if len(comparison_stats_o) > 0:\n",
        "  print(\"One element of Other_lighting part of dataset\")\n",
        "  print_comparison_stats(comparison_stats_o[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdp7JxtWFw_J"
      },
      "source": [
        "## Plotting\n",
        "In this final section, I present a clear, visual, and quantitative summary of the results obtained on a single randomly selected image from the test set. This includes:\n",
        "\n",
        "- Displaying the image with detected bounding boxes and annotated depth information.\n",
        "- Showing the corresponding depth heatmap.\n",
        "- Overlaying LiDAR point cloud data for visual reference.\n",
        "- Reporting key evaluation metrics for that image, such as model and baseline errors.\n",
        "\n",
        "This approach provides an intuitive, example-driven understanding of the model’s performance, making the results more tangible and interpretable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN3F1urEyt8v"
      },
      "source": [
        "### First Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49Rh_Go_sbD0"
      },
      "outputs": [],
      "source": [
        "num = random.randint(0,len(cam_images_test))\n",
        "transform = ToTensor()\n",
        "\n",
        "# Estrai il singolo elemento\n",
        "single_rgb_image = cam_images_test[num]\n",
        "single_depth_rel = depths_rel_test[num]\n",
        "single_depth_abs = depths_asb_test[num]\n",
        "single_points_dep_abs = points_dep_abs_test[num]\n",
        "single_depths_asb = depths_asb_test[num]\n",
        "single_desc = desc_test[num]\n",
        "\n",
        "# Preprocessing immagine RGB\n",
        "rgb_tensor = transform(single_rgb_image).to(device)\n",
        "# Add batch dimension for single image\n",
        "rgb_batch = rgb_tensor.unsqueeze(0) # (1, 3, H, W)\n",
        "\n",
        "# Preprocessing depth relative (ensure it's a tensor on the correct device)\n",
        "if not isinstance(single_depth_rel, torch.Tensor):\n",
        "    d_rel_tensor = torch.tensor(single_depth_rel).float().to(device)\n",
        "else:\n",
        "    d_rel_tensor = single_depth_rel.float().to(device)\n",
        "\n",
        "# Add batch dimension for single depth map\n",
        "d_rel_batch = d_rel_tensor.unsqueeze(0).unsqueeze(1) # (1, 1, H, W)\n",
        "\n",
        "# Concatenazione (this results in 4 channels)\n",
        "input_tensor = torch.cat([rgb_batch, d_rel_batch], dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG3gCnKpFiyX",
        "outputId": "cf0b11bb-3e2b-4bbd-89ef-b3f195b692df"
      },
      "outputs": [],
      "source": [
        "print(single_desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6XHapHqPcZ9",
        "outputId": "5a16b1b6-8c40-4567-afbe-e749125b28f0"
      },
      "outputs": [],
      "source": [
        "model_Scale.eval()\n",
        "with torch.no_grad():\n",
        "    a_b_c = model_Scale(input_tensor)\n",
        "\n",
        "# Calculate the loss using the predicted a_b and the original data\n",
        "loss = affine_depth_loss(a_b_c, [depths_rel_test[num]], [points_dep_abs_test[num]], [depths_asb_test[num]])\n",
        "print(f\"Test Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLTvlGts3FPB",
        "outputId": "134aced1-f510-4abb-9393-5a48186d3097"
      },
      "outputs": [],
      "source": [
        "target_a_b_tensor = torch.tensor([params_depth_test[num]], dtype=torch.float32).to(device)\n",
        "loss = affine_depth_loss_2d(target_a_b_tensor, [depths_rel_test[num]], [points_dep_abs_test[num]], [depths_asb_test[num]])\n",
        "print(f\"Test Loss with 3D Polyfit : {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg2i6bfs1LmP",
        "outputId": "69715322-7b73-491a-d9f5-66a0e25d8508"
      },
      "outputs": [],
      "source": [
        "target_a_b_c_tensor = torch.tensor([params_depth_test_3d[num]], dtype=torch.float32).to(device)\n",
        "loss = affine_depth_loss(target_a_b_c_tensor, [depths_rel_test[num]], [points_dep_abs_test[num]], [depths_asb_test[num]])\n",
        "print(f\"Test Loss with 3D Polyfit : {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "pwrQ4DrV49O8",
        "outputId": "5b00ae13-4c35-4240-ab9d-607e62df1e3e"
      },
      "outputs": [],
      "source": [
        "depthScaled = scaleDepthTo255(single_depth_rel)\n",
        "plt.imshow(depthScaled)\n",
        "a_b_c_t = a_b_c.cpu().numpy()\n",
        "a = a_b_c_t[0][0]\n",
        "b = a_b_c_t[0][1]\n",
        "c = a_b_c_t[0][2]\n",
        "a_poly = params_depth_test[num][0]\n",
        "b_poly = params_depth_test[num][1]\n",
        "a_poly_3d = params_depth_test_3d[num][0]\n",
        "b_poly_3d = params_depth_test_3d[num][1]\n",
        "c_poly_3d = params_depth_test_3d[num][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "G2X8nDHuQO63",
        "outputId": "0295a80e-ed32-497c-b906-6c96e172f5a4"
      },
      "outputs": [],
      "source": [
        "results = yolo(single_rgb_image)\n",
        "results[0].show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1wJZaLRoMhB"
      },
      "outputs": [],
      "source": [
        "input_boxs = []\n",
        "for result in results[0]:\n",
        "  boxes_tensor = result.boxes.xyxy\n",
        "\n",
        "  # Converti in NumPy array\n",
        "  boxes_np = boxes_tensor.cpu().detach().numpy()\n",
        "\n",
        "  # Se ti serve solo il primo box, ad esempio:\n",
        "  input_box = boxes_np[0]\n",
        "\n",
        "  input_boxs.append(input_box)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "SK1Q2kLiosDE",
        "outputId": "73546c26-76c1-4f00-f37f-50da548e20f7"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth(single_rgb_image,single_depth_rel.detach().cpu().numpy(),input_boxs,\n",
        "    a=a_poly,\n",
        "    b=b_poly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "JmvpJpistnZ7",
        "outputId": "2aeb9180-064a-4606-f71b-f4c157438cf2"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth_3d(single_rgb_image,single_depth_rel.detach().cpu().numpy(),input_boxs,\n",
        "    a=a_poly_3d,\n",
        "    b=b_poly_3d,\n",
        "    c=c_poly_3d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "gth-6r1K5iJq",
        "outputId": "146052be-840d-49c0-9551-19eb3b592894"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth_3d(single_rgb_image,single_depth_rel.detach().cpu().numpy(),input_boxs,\n",
        "    a=a,\n",
        "    b=b,\n",
        "    c=c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "qHPn7vVioxMn",
        "outputId": "4d30b7ea-d50b-437d-9741-43c0d38f4220"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth2(single_rgb_image,single_depth_rel.detach().cpu().numpy(),input_boxs,\n",
        "    a=a_poly,\n",
        "    b=b_poly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "0uOIrY-vpUV1",
        "outputId": "4290934b-cd61-42af-f9a5-8ce0e3324cf4"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth3(single_rgb_image,single_depth_rel.detach().cpu().numpy(),input_boxs,\n",
        "    a=a_poly_3d,\n",
        "    b=b_poly_3d,\n",
        "    c=c_poly_3d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "yDBKXTaetUuU",
        "outputId": "31da8255-8463-4a6b-e390-ea5f460f362a"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth3(single_rgb_image,single_depth_rel.detach().cpu().numpy(),input_boxs,\n",
        "    a=a,\n",
        "    b=b,\n",
        "    c=c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0vz4mJm4YNrW",
        "outputId": "e1db835e-3d9f-42b6-b98a-cacb84ba06cb"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth_rel_abs(image= single_rgb_image, depth_rel= single_depth_rel.detach().cpu().numpy(),\n",
        "                        input_boxes= input_boxs,\n",
        "                        a=a,\n",
        "                        b=b,\n",
        "                        c=c,\n",
        "                        points_dep_abs=single_points_dep_abs,\n",
        "                        depths_abs=single_depths_asb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kHuqpIKTZDOK",
        "outputId": "86e65d9e-e238-4a77-a731-32d695490984"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth_rel_abs(image= single_rgb_image, depth_rel= single_depth_rel.detach().cpu().numpy(),\n",
        "                        input_boxes= input_boxs,\n",
        "                        a=a_poly_3d,\n",
        "                        b=b_poly_3d,\n",
        "                        c=c_poly_3d,\n",
        "                        points_dep_abs=single_points_dep_abs,\n",
        "                        depths_abs=single_depths_asb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFM97CglypAm"
      },
      "source": [
        "### Second Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5IFo_ELylzB"
      },
      "outputs": [],
      "source": [
        "num = random.randint(0,len(cam_images_test))\n",
        "transform = ToTensor()\n",
        "\n",
        "# Estrai il singolo elemento\n",
        "single_rgb_image = cam_images_test[num]\n",
        "single_depth_rel = depths_rel_test[num]\n",
        "single_depth_abs = depths_asb_test[num]\n",
        "single_points_dep_abs = points_dep_abs_test[num]\n",
        "single_depths_asb = depths_asb_test[num]\n",
        "single_desc = desc_test[num]\n",
        "\n",
        "# Preprocessing immagine RGB\n",
        "rgb_tensor = transform(single_rgb_image).to(device)\n",
        "# Add batch dimension for single image\n",
        "rgb_batch = rgb_tensor.unsqueeze(0) # (1, 3, H, W)\n",
        "\n",
        "# Preprocessing depth relative (ensure it's a tensor on the correct device)\n",
        "if not isinstance(single_depth_rel, torch.Tensor):\n",
        "    d_rel_tensor = torch.tensor(single_depth_rel).float().to(device)\n",
        "else:\n",
        "    d_rel_tensor = single_depth_rel.float().to(device)\n",
        "\n",
        "# Add batch dimension for single depth map\n",
        "d_rel_batch = d_rel_tensor.unsqueeze(0).unsqueeze(1) # (1, 1, H, W)\n",
        "\n",
        "# Concatenazione (this results in 4 channels)\n",
        "input_tensor = torch.cat([rgb_batch, d_rel_batch], dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9I2bVpaylzC",
        "outputId": "2127d9e2-d403-4a89-b277-858339b066f4"
      },
      "outputs": [],
      "source": [
        "print(single_desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZc50Mj1ylzD",
        "outputId": "776d2272-c007-48ce-fd68-f35fa01cae82"
      },
      "outputs": [],
      "source": [
        "model_Scale.eval()\n",
        "with torch.no_grad():\n",
        "    a_b_c = model_Scale(input_tensor)\n",
        "\n",
        "# Calculate the loss using the predicted a_b and the original data\n",
        "loss = affine_depth_loss(a_b_c, [depths_rel_test[num]], [points_dep_abs_test[num]], [depths_asb_test[num]])\n",
        "print(f\"Test Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iyqw0uNZylzE",
        "outputId": "eb6d7285-0ceb-4343-f876-060154e94263"
      },
      "outputs": [],
      "source": [
        "target_a_b_tensor = torch.tensor([params_depth_test[num]], dtype=torch.float32).to(device)\n",
        "loss = affine_depth_loss_2d(target_a_b_tensor, [depths_rel_test[num]], [points_dep_abs_test[num]], [depths_asb_test[num]])\n",
        "print(f\"Test Loss with 3D Polyfit : {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-Mx3M83ylzE",
        "outputId": "bb123b30-c7b4-4c70-8084-0decbfa9dd54"
      },
      "outputs": [],
      "source": [
        "target_a_b_c_tensor = torch.tensor([params_depth_test_3d[num]], dtype=torch.float32).to(device)\n",
        "loss = affine_depth_loss(target_a_b_c_tensor, [depths_rel_test[num]], [points_dep_abs_test[num]], [depths_asb_test[num]])\n",
        "print(f\"Test Loss with 3D Polyfit : {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "b8oxzMarylzE",
        "outputId": "6ed60062-a5b6-4050-9890-6673b2547488"
      },
      "outputs": [],
      "source": [
        "depthScaled = scaleDepthTo255(single_depth_rel)\n",
        "plt.imshow(depthScaled)\n",
        "a_b_c_t = a_b_c.cpu().numpy()\n",
        "a = a_b_c_t[0][0]\n",
        "b = a_b_c_t[0][1]\n",
        "c = a_b_c_t[0][2]\n",
        "a_poly = params_depth_test[num][0]\n",
        "b_poly = params_depth_test[num][1]\n",
        "a_poly_3d = params_depth_test_3d[num][0]\n",
        "b_poly_3d = params_depth_test_3d[num][1]\n",
        "c_poly_3d = params_depth_test_3d[num][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "vvkRxy46ylzE",
        "outputId": "d868087f-fe88-4ea6-ccdf-b0e536110704"
      },
      "outputs": [],
      "source": [
        "results = yolo(single_rgb_image)\n",
        "results[0].show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4H7KOnHylzF"
      },
      "outputs": [],
      "source": [
        "input_boxs = []\n",
        "for result in results[0]:\n",
        "  boxes_tensor = result.boxes.xyxy\n",
        "\n",
        "  # Converti in NumPy array\n",
        "  boxes_np = boxes_tensor.cpu().detach().numpy()\n",
        "\n",
        "  # Se ti serve solo il primo box, ad esempio:\n",
        "  input_box = boxes_np[0]\n",
        "\n",
        "  input_boxs.append(input_box)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "cSh2ee-aylzF",
        "outputId": "f5296ff8-5ab4-4bee-be02-58afd1be8c31"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth(single_rgb_image,single_depth_rel.detach().cpu().numpy(),input_boxs,\n",
        "    a=a_poly,\n",
        "    b=b_poly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "exqw6XePylzF",
        "outputId": "83ccb3d7-b9c8-4355-8fe0-2cb7b2147af0"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth_3d(single_rgb_image,single_depth_rel.detach().cpu().numpy(),input_boxs,\n",
        "    a=a_poly_3d,\n",
        "    b=b_poly_3d,\n",
        "    c=c_poly_3d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "1NbEEhS0ylzF",
        "outputId": "4be46ec7-3a08-4f72-a718-cb69056b33c0"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth_3d(single_rgb_image,single_depth_rel.detach().cpu().numpy(),input_boxs,\n",
        "    a=a,\n",
        "    b=b,\n",
        "    c=c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "gEEPsJAaylzF",
        "outputId": "4b2d985f-52e1-4c29-8ad0-2cf69d86e411"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth2(single_rgb_image,single_depth_rel.detach().cpu().numpy(),input_boxs,\n",
        "    a=a_poly,\n",
        "    b=b_poly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "incFfNEmylzG",
        "outputId": "6116da48-83ac-4ffb-afc3-99e884c48a7e"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth3(single_rgb_image,single_depth_rel.detach().cpu().numpy(),input_boxs,\n",
        "    a=a_poly_3d,\n",
        "    b=b_poly_3d,\n",
        "    c=c_poly_3d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "wyqbWPCHylzG",
        "outputId": "13349db1-888b-40c2-8c22-292f6c0b128f"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth3(single_rgb_image,single_depth_rel.detach().cpu().numpy(),input_boxs,\n",
        "    a=a,\n",
        "    b=b,\n",
        "    c=c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "NRREKXciylzG",
        "outputId": "c9edae42-0118-44c5-857b-0284906b52d4"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth_rel_abs(image= single_rgb_image, depth_rel= single_depth_rel.detach().cpu().numpy(),\n",
        "                        input_boxes= input_boxs,\n",
        "                        a=a,\n",
        "                        b=b,\n",
        "                        c=c,\n",
        "                        points_dep_abs=single_points_dep_abs,\n",
        "                        depths_abs=single_depths_asb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "nkraYjjkylzG",
        "outputId": "e10200c3-16a6-4215-cc1b-eaa495ecd066"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth_rel_abs(image= single_rgb_image, depth_rel= single_depth_rel.detach().cpu().numpy(),\n",
        "                        input_boxes= input_boxs,\n",
        "                        a=a_poly_3d,\n",
        "                        b=b_poly_3d,\n",
        "                        c=c_poly_3d,\n",
        "                        points_dep_abs=single_points_dep_abs,\n",
        "                        depths_abs=single_depths_asb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ85Pu9hyyHz"
      },
      "source": [
        "### Third Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J26N8_bvy0-9"
      },
      "outputs": [],
      "source": [
        "num = random.randint(0,len(cam_images_test))\n",
        "transform = ToTensor()\n",
        "\n",
        "# Estrai il singolo elemento\n",
        "single_rgb_image = cam_images_test[num]\n",
        "single_depth_rel = depths_rel_test[num]\n",
        "single_depth_abs = depths_asb_test[num]\n",
        "single_points_dep_abs = points_dep_abs_test[num]\n",
        "single_depths_asb = depths_asb_test[num]\n",
        "single_desc = desc_test[num]\n",
        "\n",
        "# Preprocessing immagine RGB\n",
        "rgb_tensor = transform(single_rgb_image).to(device)\n",
        "# Add batch dimension for single image\n",
        "rgb_batch = rgb_tensor.unsqueeze(0) # (1, 3, H, W)\n",
        "\n",
        "# Preprocessing depth relative (ensure it's a tensor on the correct device)\n",
        "if not isinstance(single_depth_rel, torch.Tensor):\n",
        "    d_rel_tensor = torch.tensor(single_depth_rel).float().to(device)\n",
        "else:\n",
        "    d_rel_tensor = single_depth_rel.float().to(device)\n",
        "\n",
        "# Add batch dimension for single depth map\n",
        "d_rel_batch = d_rel_tensor.unsqueeze(0).unsqueeze(1) # (1, 1, H, W)\n",
        "\n",
        "# Concatenazione (this results in 4 channels)\n",
        "input_tensor = torch.cat([rgb_batch, d_rel_batch], dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1ArtX05y0--",
        "outputId": "60b8139f-3428-4113-e4e8-4fa52d3cceca"
      },
      "outputs": [],
      "source": [
        "print(single_desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMLEdKVIy0-_",
        "outputId": "d3211b3d-c6b2-4977-ca0b-e38b83ccdbf7"
      },
      "outputs": [],
      "source": [
        "model_Scale.eval()\n",
        "with torch.no_grad():\n",
        "    a_b_c = model_Scale(input_tensor)\n",
        "\n",
        "# Calculate the loss using the predicted a_b and the original data\n",
        "loss = affine_depth_loss(a_b_c, [depths_rel_test[num]], [points_dep_abs_test[num]], [depths_asb_test[num]])\n",
        "print(f\"Test Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZejiPMb9y0-_",
        "outputId": "ce22e22c-fe5b-49b2-ccd2-332f3456ae39"
      },
      "outputs": [],
      "source": [
        "target_a_b_tensor = torch.tensor([params_depth_test[num]], dtype=torch.float32).to(device)\n",
        "loss = affine_depth_loss_2d(target_a_b_tensor, [depths_rel_test[num]], [points_dep_abs_test[num]], [depths_asb_test[num]])\n",
        "print(f\"Test Loss with 3D Polyfit : {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWkHDT4Hy0-_",
        "outputId": "99822663-4b53-406d-8972-24164cba236e"
      },
      "outputs": [],
      "source": [
        "target_a_b_c_tensor = torch.tensor([params_depth_test_3d[num]], dtype=torch.float32).to(device)\n",
        "loss = affine_depth_loss(target_a_b_c_tensor, [depths_rel_test[num]], [points_dep_abs_test[num]], [depths_asb_test[num]])\n",
        "print(f\"Test Loss with 3D Polyfit : {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "_kFwGHzfy0-_",
        "outputId": "17d37f25-aba2-4413-af2b-52af368fb99a"
      },
      "outputs": [],
      "source": [
        "depthScaled = scaleDepthTo255(single_depth_rel)\n",
        "plt.imshow(depthScaled)\n",
        "a_b_c_t = a_b_c.cpu().numpy()\n",
        "a = a_b_c_t[0][0]\n",
        "b = a_b_c_t[0][1]\n",
        "c = a_b_c_t[0][2]\n",
        "a_poly = params_depth_test[num][0]\n",
        "b_poly = params_depth_test[num][1]\n",
        "a_poly_3d = params_depth_test_3d[num][0]\n",
        "b_poly_3d = params_depth_test_3d[num][1]\n",
        "c_poly_3d = params_depth_test_3d[num][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "3OCjK452y0_A",
        "outputId": "3ed96e90-636a-4d22-878d-dd8601ec5d8a"
      },
      "outputs": [],
      "source": [
        "results = yolo(single_rgb_image)\n",
        "results[0].show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRBRmH7Fy0_A"
      },
      "outputs": [],
      "source": [
        "input_boxs = []\n",
        "for result in results[0]:\n",
        "  boxes_tensor = result.boxes.xyxy\n",
        "\n",
        "  # Converti in NumPy array\n",
        "  boxes_np = boxes_tensor.cpu().detach().numpy()\n",
        "\n",
        "  # Se ti serve solo il primo box, ad esempio:\n",
        "  input_box = boxes_np[0]\n",
        "\n",
        "  input_boxs.append(input_box)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "AA2qknOEy0_A",
        "outputId": "e452c4ca-231e-4411-f6bb-3ecf76812786"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth(single_rgb_image,single_depth_rel.detach().cpu().numpy(),input_boxs,\n",
        "    a=a_poly,\n",
        "    b=b_poly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "PiNMWA6Fy0_A",
        "outputId": "b56281e7-227a-4a49-ecee-a10113121d73"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth_3d(single_rgb_image,single_depth_rel.detach().cpu().numpy(),input_boxs,\n",
        "    a=a_poly_3d,\n",
        "    b=b_poly_3d,\n",
        "    c=c_poly_3d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "8UWEBHX2y0_A",
        "outputId": "f36cc27b-066f-4015-d429-965102f53952"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth_3d(single_rgb_image,single_depth_rel.detach().cpu().numpy(),input_boxs,\n",
        "    a=a,\n",
        "    b=b,\n",
        "    c=c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "tvQXBR1Ey0_A",
        "outputId": "83ac0c50-abeb-40a2-c4c5-e42b87556b93"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth2(single_rgb_image,single_depth_rel.detach().cpu().numpy(),input_boxs,\n",
        "    a=a_poly,\n",
        "    b=b_poly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "RLXf2LOty0_B",
        "outputId": "c1518c6f-b686-4d7f-bb02-c89c8c6ab973"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth3(single_rgb_image,single_depth_rel.detach().cpu().numpy(),input_boxs,\n",
        "    a=a_poly_3d,\n",
        "    b=b_poly_3d,\n",
        "    c=c_poly_3d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "7c0ym7m2y0_B",
        "outputId": "5b0b4777-4a89-44b8-dc0e-08d229e4d1db"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth3(single_rgb_image,single_depth_rel.detach().cpu().numpy(),input_boxs,\n",
        "    a=a,\n",
        "    b=b,\n",
        "    c=c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t4Hb_olwy0_B",
        "outputId": "95eda31d-a398-477f-f449-fed1ca740457"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth_rel_abs(image= single_rgb_image, depth_rel= single_depth_rel.detach().cpu().numpy(),\n",
        "                        input_boxes= input_boxs,\n",
        "                        a=a,\n",
        "                        b=b,\n",
        "                        c=c,\n",
        "                        points_dep_abs=single_points_dep_abs,\n",
        "                        depths_abs=single_depths_asb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DpbW2zF3y0_B",
        "outputId": "f2387a7f-1340-40fb-e55b-566354ee0602"
      },
      "outputs": [],
      "source": [
        "plot_with_box_depth_rel_abs(image= single_rgb_image, depth_rel= single_depth_rel.detach().cpu().numpy(),\n",
        "                        input_boxes= input_boxs,\n",
        "                        a=a_poly_3d,\n",
        "                        b=b_poly_3d,\n",
        "                        c=c_poly_3d,\n",
        "                        points_dep_abs=single_points_dep_abs,\n",
        "                        depths_abs=single_depths_asb)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "kGKI5B-N5Yu1",
        "OVX1JzRs5eDq",
        "Qjol_wtd6eeg",
        "DT66wzFp9d53",
        "FYwLqILa6xbk",
        "09Clh_m6_rVA",
        "sGorIpXgAl5f",
        "iig5AhCFBEFd"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "11e197800d194a9dac3e9bbce26161e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_e97ce33b0ebd47098b4da105d39a33b3",
            "placeholder": "​",
            "style": "IPY_MODEL_e95632974ed94c6c88e4e982a37453e7",
            "tabbable": null,
            "tooltip": null,
            "value": "preprocessor_config.json: 100%"
          }
        },
        "1d07db9a91fb497c993d3001c16284dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2640ef34b8f04a54a11cfcc3e476a589": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55ffbaf581bd42b7826f1f456b926cd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "573bced22f064b41b2bacf532139e89e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58814fb49e6547a981299c0599201800": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ffabc59a6bd43b2aab04831ec067fdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_9215ca3450d94cd5bbc69257c1c2ba77",
            "placeholder": "​",
            "style": "IPY_MODEL_98c5c5f57ef84a2b952b3b15ab4c69c4",
            "tabbable": null,
            "tooltip": null,
            "value": " 775/775 [00:00&lt;00:00, 78.3kB/s]"
          }
        },
        "621cebb99aa54a5797bd0fcc206a14c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b7b0bafa91c4e7cb1bd946148a1ef39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d021abe4e19484485be145df6d7f825": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "9215ca3450d94cd5bbc69257c1c2ba77": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "938ff96efcc043d7bb980721a54fe5af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98c5c5f57ef84a2b952b3b15ab4c69c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "9c37b82accb846bb8c530cc5a15e8a78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_abb3d148fb3b4d828caa0c91c6349839",
            "max": 99173660,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_573bced22f064b41b2bacf532139e89e",
            "tabbable": null,
            "tooltip": null,
            "value": 99173660
          }
        },
        "a1798fca83ed4d7b93d14fb265969516": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4ed3b5a744b412d800072bab9086e4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11e197800d194a9dac3e9bbce26161e8",
              "IPY_MODEL_bff1630a5f4944b2937c2b96bf1cc387",
              "IPY_MODEL_5ffabc59a6bd43b2aab04831ec067fdf"
            ],
            "layout": "IPY_MODEL_b29a926622274da598485147c76abf3b",
            "tabbable": null,
            "tooltip": null
          }
        },
        "a5694f4a15714eaab37888356fb3f140": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_c6ef629fe0a8444997628d9e23f8b869",
            "placeholder": "​",
            "style": "IPY_MODEL_7d021abe4e19484485be145df6d7f825",
            "tabbable": null,
            "tooltip": null,
            "value": " 950/950 [00:00&lt;00:00, 82.0kB/s]"
          }
        },
        "abb3d148fb3b4d828caa0c91c6349839": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b29a926622274da598485147c76abf3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b422c6253c6e44329998c158d6829e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_621cebb99aa54a5797bd0fcc206a14c5",
            "placeholder": "​",
            "style": "IPY_MODEL_d206236ac02445edb7f481e1cba4c011",
            "tabbable": null,
            "tooltip": null,
            "value": "model.safetensors: 100%"
          }
        },
        "bc84acfda41e45728ab41abbe2d2e836": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b422c6253c6e44329998c158d6829e96",
              "IPY_MODEL_9c37b82accb846bb8c530cc5a15e8a78",
              "IPY_MODEL_d4eb2f2adeaf4685aa703e964d2872ab"
            ],
            "layout": "IPY_MODEL_58814fb49e6547a981299c0599201800",
            "tabbable": null,
            "tooltip": null
          }
        },
        "bff1630a5f4944b2937c2b96bf1cc387": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_a1798fca83ed4d7b93d14fb265969516",
            "max": 775,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b7b0bafa91c4e7cb1bd946148a1ef39",
            "tabbable": null,
            "tooltip": null,
            "value": 775
          }
        },
        "c20d3aa7ea9a46e1bbf1ca6a46fbd4e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_e149fb09fe3844af8b8d9f93663fad8e",
            "max": 950,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cbdada9f5d3b4fc890fa49127d2f936c",
            "tabbable": null,
            "tooltip": null,
            "value": 950
          }
        },
        "c6ef629fe0a8444997628d9e23f8b869": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbdada9f5d3b4fc890fa49127d2f936c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d206236ac02445edb7f481e1cba4c011": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "d4eb2f2adeaf4685aa703e964d2872ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_2640ef34b8f04a54a11cfcc3e476a589",
            "placeholder": "​",
            "style": "IPY_MODEL_d514c79fe11349b0bea520ef8dd43b41",
            "tabbable": null,
            "tooltip": null,
            "value": " 99.2M/99.2M [00:00&lt;00:00, 263MB/s]"
          }
        },
        "d514c79fe11349b0bea520ef8dd43b41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "e149fb09fe3844af8b8d9f93663fad8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e95632974ed94c6c88e4e982a37453e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "e97ce33b0ebd47098b4da105d39a33b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3036fc7113542f6a6ec50b6133868a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_1d07db9a91fb497c993d3001c16284dd",
            "placeholder": "​",
            "style": "IPY_MODEL_55ffbaf581bd42b7826f1f456b926cd9",
            "tabbable": null,
            "tooltip": null,
            "value": "config.json: 100%"
          }
        },
        "fbad99b875da43039c268f2cc4bd7229": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3036fc7113542f6a6ec50b6133868a2",
              "IPY_MODEL_c20d3aa7ea9a46e1bbf1ca6a46fbd4e8",
              "IPY_MODEL_a5694f4a15714eaab37888356fb3f140"
            ],
            "layout": "IPY_MODEL_938ff96efcc043d7bb980721a54fe5af",
            "tabbable": null,
            "tooltip": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
