# Introduction

This study introduces a comprehensive pipeline for evaluating depth estimation and transformation models applied to the MAN TruckScenes mini dataset, with a systematic comparative analysis of three ScaleBiasModel variants characterized by different computational complexities: a **Light ScaleBiasModel** optimized for computational efficiency with reduced parameters for real-time applications, a **Moderate ScaleBiasModel** offering balanced performance and computational cost, and a **Heavy ScaleBiasModel** with high-capacity architecture to maximize predictive accuracy. The experimental framework implements an integrated system that combines deep learning techniques, classical regression, and computer vision methodologies, following established neural model scaling principles to analyze the fundamental trade-off between model complexity and performance. The complete pipeline encompasses the acquisition and preprocessing of the MAN TruckScenes mini dataset with RGB images and LiDAR point clouds, relative depth estimation using the pre-trained Depth-Anything V2 transformer model trained on 595K labeled synthetic and 62M unlabeled real images, extraction of affine transformation parameters through linear and quadratic polynomial regression techniques, definition and training of three scalable ScaleBiasHead neural network configurations with combined loss functions, multi-metric comparative evaluation against polynomial regression baselines (2D and 3D polyfit) under various operating conditions using RMSE, MAE, and depth consistency measures, validation through YOLO-based object detection for granular per-bounding-box error analysis compared to LiDAR ground truth, and detailed per-image visual analysis with depth map overlays and quantitative comparisons between the three ScaleBiasModel variants and polynomial baselines, establishing a rigorous, interpretable, and scenario-aware evaluation framework particularly relevant for real-world autonomous driving applications in commercial transport scenarios.

## Architectures of the Three ScaleBiasHead Models

The three implemented ScaleBiasHead models feature distinctive architectures designed to balance computational efficiency and predictive accuracy in depth estimation. The **Light_ScaleBiasHead** adopts a minimalist design that uses reduced channels (progressively from 4 to 32 channels) and components optimized for efficiency: it employs a LightSEBlock with a reduced reduction factor of 8, a SimplifiedMultiScale with only two branches (1x1 and 3x3 convolution) for multi-scale feature fusion, and an EfficientDepthwiseConv that separates depthwise and pointwise operations to reduce computational complexity, culminating in a simplified fully-connected head that directly maps from 32 to 3 output parameters. The **Moderate_ScaleBiasHead** implements a balanced approach with a sequential convolutional block that uses wider kernels (initially 5x5) followed by standard 3x3 convolutions, processing channels from 4 to 32 through two stages of max pooling, and ends with an AdaptiveAvgPool2d that reduces feature maps to a fixed size (4x4) before a more robust fully-connected head with 128 hidden neurons and 0.5 dropout for regularization. The **Heavy_ScaleBiasHead** represents the most sophisticated architecture, incorporating an advanced MultiScaleFeatureFusion with four parallel branches (1x1, 3x3, 5x5 convolution, and max pooling followed by 1x1 convolution) to capture features at different spatial scales, a standard SEBlock with a reduction factor of 16 for channel attention mechanism, a DepthwiseSeparableConv with separate batch normalization for depthwise and pointwise operations, and a two-stage fully-connected head (64→32→3) with dropout to maximize the model's representational capacity.

## DepthAnything2
DepthAnything2 Small is a lightweight yet powerful model for monocular depth estimation, meaning it predicts depth information from a single image. Built on the DPT architecture with a DINOv2 backbone, it processes visual input using transformer-based methods. The model is trained on a massive dataset of 595,000 synthetic labeled images and over 62 million real unlabeled images, which allows it to achieve highly accurate and robust depth predictions across diverse scenes. Compared to previous versions and Stable Diffusion-based models, DepthAnything2 Small is significantly faster (about 10 times) and more efficient, while still capturing fine-grained details in depth maps. With only 24.8 million parameters, it is optimized for speed and resource efficiency, making it suitable for real-time applications like robotics, augmented reality, and autonomous navigation, especially where computational resources are limited.

## YOLO11n
YOLO11n is the nano-sized, ultra-lightweight variant of the YOLO11 object detection model, designed for high efficiency and real-time performance on resource-constrained devices. Despite its compact architecture, YOLO11n leverages key innovations from the YOLO11 family—including enhanced backbone and neck components, advanced spatial attention mechanisms, and optimized Convolution-BatchNorm-SiLU (CBS) blocks—to achieve strong detection accuracy with minimal computational overhead. Like all YOLO11 models, YOLO11n supports a broad range of computer vision tasks such as object detection, instance segmentation, image classification, pose estimation, and oriented object detection, making it highly versatile for edge deployments and real-time applications.

## MAN TruckScenes Mini
The MAN TruckScenes Mini dataset offers rich multimodal annotations from a comprehensive sensor suite specifically designed for autonomous trucking. Each scene includes synchronized data from 4 RGB cameras, 6 lidar sensors, and 6 radar sensors, providing dense spatial coverage and robust perception in diverse conditions. The lidar sensors deliver high-resolution 3D point clouds, while the radar sensors—featuring 4D capability—capture both range-azimuth and elevation information, yielding around 2,600 points per sample with nearly 360-degree coverage. Additionally, the dataset incorporates precise vehicle state and localization data from two IMUs and a high-precision RTK-GNSS unit. All objects within a range of over 230 meters are manually annotated with 3D bounding boxes, covering 27 object classes and 15 attributes, and are tracked throughout each scene. These detailed annotations support a wide range of perception tasks, including 3D object detection, tracking, and sensor fusion research for autonomous trucking applications.

# Flow of the scrpits 

The implementation prioritizes computational efficiency through strategic memory management and device optimization. Automatic GPU detection enables accelerated processing when available, while explicit memory cleanup prevents accumulation during batch processing. The training pipeline supports variable batch sizes with memory-conscious processing, enabling operation on systems with different computational capabilities.
This approach is necessary to address issues related to limited RAM in the Colab environment.

## Dataset Foundation and Preparation
There is strategic train-test split using specific scene selections to ensure balanced representation across different operational conditions. Training scenes (0, 2, 3, 6, 8) encompass diverse environments including terminal areas, city streets, and highways under clear, overcast, and rainy conditions  . Test scenes (1, 4, 5, 7, 9) are deliberately chosen to include challenging scenarios such as snow conditions, roadworks, overpass structures, and various lighting conditions including twilight and alternative illumination  . For computational efficiency, the test set is subsampled to 30% of its original size while maintaining representational diversity.
From the dataset using TruckScenes API e some modification for every sample is extracted: the image, the mapping point-lidar_mesuraments and the description of the scene.
It's important to notice that for every img there is a limited number of lidar-points that we can use. 

## Relative Depth Estimation Through Transformer Architecture
The pipeline employs Depth-Anything V2, a state-of-the-art monocular depth estimation model that leverages transformer architecture for robust depth prediction.
During preprocessing, all input images are processed through the Depth-Anything V2 pipeline to generate relative depth heatmaps . These heatmaps capture spatial depth relationships but require transformation to absolute depth measurements for practical applications . The relative depth maps are then inverted using the mathematical operation `depth_rel = depth_rel_max - depth_rel_img` to align with conventional depth representations where larger values correspond to greater distances.

## Polynomial Regression for Affine Parameter Extraction
Two distinct polynomial regression are implementes, that approaches to establish baseline relationships between relative and absolute depth measurements . Linear regression models the relationship as $$D_{abs} = a \cdot D_{rel} + b$$, providing a simple two-parameter transformation . Quadratic regression extends this to $$D_{abs} = a \cdot D_{rel}^2 + b \cdot D_{rel} + c$$, capturing non-linear depth relationships through three parameters .Both approaches utilize NumPy's `polyfit` function to compute optimal coefficients through least-squares optimization. 
The polynomial fitting process involves extracting valid pixel coordinates from LiDAR point cloud projections onto the camera image plane, filtering out invalid depth values, and computing regression coefficients that minimize the mean squared error between predicted and measured absolute depths. These polynomial parameters serve dual purposes: establishing baseline performance metrics and providing supervision signals for neural network training.

## Neural Network Architecture and Training Strategy
The core innovation lies in the ScaleBiasHead neural network, a custom architecture designed to predict affine transformation parameters through learned feature representations  . This network processes 4-channel input tensors combining RGB imagery with relative depth information to output three affine parameters (a, b, c) for quadratic depth transformation. The architectures of 3 models are described in the previous paragraph.

Training employs a hybrid loss function combining depth prediction accuracy with parameter regularization. 
The depth loss component uses mean squared error to measure differences between transformed relative depths and LiDAR ground truth measurements  . 
The parameter loss applies Huber loss to encourage predicted affine parameters to remain close to polynomial regression baselines, providing stability and physical plausibility . 
The total loss function is formulated as:
$$L_{total} = L_{depth} + 0.5 \times L_{parameter}$$
Early stopping with patience=5 prevents overfitting by monitoring validation loss and terminating training when performance plateaus.

## Comprehensive Evaluation Methodology
The evaluation framework implements a three-tier approach progressing from abstract loss metrics to physically interpretable measurements.
Global evaluation computes test losses across the entire dataset, comparing neural network predictions against polynomial baselines using standardized loss functions. 
Condition-specific evaluation filters the dataset by lighting conditions (illuminated, dark, other_lighting) to assess performance under varying environmental scenarios.
The most innovative aspect involves object-level evaluation through YOLO integration, providing tangible metrics expressed in physical units. YOLO11n object detection generates bounding boxes for detected objects, enabling per-object depth analysis . For each bounding box, the system computes depth statistics using both model-predicted and polynomial-derived affine parameters, comparing these against LiDAR ground truth measurements within the box boundaries  .

Key evaluation metrics include mean absolute error measured in meters, per-point error normalized by LiDAR point density, and comparative analysis between different approaches. 
This progression from dataset-wide metrics to object-specific measurements provides comprehensive insight into model performance across different scales and conditions. 

# Comparative Analysis

**Light Model: Excellence in Efficiency and Versatility**  
The Light model stands out as a particularly effective solution for real-time applications, achieving the best overall mean absolute error of 1.711m among all neural configurations. Its architecture, optimized for computational efficiency, does not compromise performance; in fact, it excels in mixed lighting conditions with errors as low as 0.787m, outperforming polynomial regression by 9.35x. The model also demonstrates superior capabilities with sparse datasets, achieving errors of 3.791m compared to 10.223m for classical approaches, highlighting the effectiveness of learned neural representations in compensating for limited geometric data density. This combination of computational speed and predictive accuracy makes the Light model ideal for commercial deployment where responsiveness and reliability are priorities.

**Moderate Model: Strategic Balance and Robustness**  
The Moderate model represents a strategically balanced configuration that offers a smart compromise between architectural complexity and operational performance. While it shows greater variability compared to classical approaches, the model is particularly effective with small datasets, achieving errors of 5.961m versus 9.530m for polynomial regression—a 60% improvement. Its intermediate architecture ensures acceptable inference times while maintaining higher representational capacity than the Light model, making it suitable for applications that require moderate but consistent performance. In mixed lighting conditions, the Moderate model remains competitive with nearly equivalent performance to classical approaches, demonstrating operational versatility in complex environmental scenarios.

**Heavy Model: Peak Performance in Optimal Conditions**  
The Heavy model is the most sophisticated configuration among the neural variants, achieving an almost perfect balance with classical approaches, outperforming them in 50.2% of cases. The high-capacity architecture particularly excels in well-lit conditions with errors of just 3.24m, demonstrating superior ability to process high-quality visual information. The Heavy model achieves outstanding results with small and medium datasets, delivering improvements of 67.9% and 51.4% respectively over polynomial regression, showcasing the power of neural feature extraction techniques. In mixed lighting, the model outperforms classical approaches in 63.7% of cases, demonstrating adaptive capabilities that effectively leverage architectural complexity to handle challenging environmental scenarios.

**Heavy Model with 100 Epochs: Specialization and Advanced Capabilities**  
The Heavy model with extended training to 100 epochs maintains distinctive strengths in specific domains, showing particular excellence with sparse datasets where it surpasses polynomial regression by 35.1%. Despite prolonged training, the model remains competitive in mixed lighting conditions, achieving nearly equivalent performance to classical approaches with errors of 8.014m versus 8.204m. Granular bounding box analysis reveals exceptional abilities in specific detections, as seen in Plot 02 where it achieves errors as low as 3.73m for high-density point clouds, significantly outperforming polynomial regression. This configuration demonstrates potential for highly specialized applications where advanced adaptive capabilities can be leveraged in controlled operational scenarios, representing a solution for deployments requiring maximum algorithmic sophistication.

## Overall Performance Overview
The overall performance comparison between the ScaleBiasModel neural network variants and polynomial regression approaches highlights distinct strengths and trade-offs for each method. The Light model stands out as the most accurate among neural networks, achieving a mean absolute error of 1.711m, while the Heavy_100_epochs model records the highest error at 7.956m, indicating that extended training does not necessarily yield better results. Polynomial regression offers remarkable consistency, with mean errors between 4.157m and 5.210m across all scenarios, and consistently achieves the lowest test loss values, underscoring its stability and reliability. Neural networks, on the other hand, demonstrate a higher sensitivity to architecture and training parameters, resulting in greater variability in performance. Notably, neural models excel with small datasets—where, for example, the Heavy model achieves a 3.893m error versus 12.113m for polynomial regression—but their performance declines with large datasets, where polynomial regression outperforms them. In low-light conditions, all neural models experience significant degradation, while polynomial regression maintains stable results. Conversely, in mixed and optimal lighting, the Light model demonstrates excellent generalization, achieving very low errors and confirming its suitability for real-time applications that require both responsiveness and adaptability.

# Practical Implications for Autonomous Driving Applications

The ScaleBiasModel variants demonstrate significant potential for real-world autonomous vehicle deployment, particularly when considering the challenging constraints under which this research was conducted . The Light model's achievement of 1.711m mean absolute error represents a promising foundation for real-time depth estimation in commercial trucking applications, where computational efficiency and responsiveness are critical for safety-critical decision making . While the neural networks show sensitivity to environmental conditions, particularly in low-light scenarios, these limitations must be contextualized within the experimental constraints of the study, including the relatively small MAN TruckScenes mini dataset, sparse LiDAR point sampling that provides limited ground truth coverage, and computational restrictions imposed by the Colab environment that prevented extensive hyperparameter optimization . The superior performance of neural models with sparse datasets (where the Heavy model achieves 67.9% improvement over polynomial regression) is particularly relevant for autonomous trucking, where sensor occlusion, weather interference, or partial sensor failures may limit data availability . Furthermore, the models' varying performance across different point cloud densities suggests that with optimized sensor fusion strategies, expanded training datasets, and dedicated computational resources for comprehensive parameter tuning, these architectures could achieve substantially improved robustness for deployment in autonomous vehicle perception systems . The framework's integration of transformer-based depth estimation with classical regression baselines provides a practical fallback mechanism that could enhance system reliability in production environments, where safety-critical applications require multiple redundant approaches to handle edge cases and sensor degradation scenarios commonly encountered in real-world autonomous driving operations.
