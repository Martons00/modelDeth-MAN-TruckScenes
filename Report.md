# Introduction

This study introduces a comprehensive pipeline for evaluating depth estimation and transformation models applied to the MAN TruckScenes mini dataset, with a systematic comparative analysis of three ScaleBiasModel variants characterized by different computational complexities: a **Light ScaleBiasModel** optimized for computational efficiency with reduced parameters for real-time applications, a **Moderate ScaleBiasModel** offering balanced performance and computational cost, and a **Heavy ScaleBiasModel** with high-capacity architecture to maximize predictive accuracy. The experimental framework implements an integrated system that combines deep learning techniques, classical regression, and computer vision methodologies, following established neural model scaling principles to analyze the fundamental trade-off between model complexity and performance. The complete pipeline encompasses the acquisition and preprocessing of the MAN TruckScenes mini dataset with RGB images and LiDAR point clouds, relative depth estimation using the pre-trained Depth-Anything V2 transformer model trained on 595K labeled synthetic and 62M unlabeled real images, extraction of affine transformation parameters through linear and quadratic polynomial regression techniques, definition and training of three scalable ScaleBiasHead neural network configurations with combined loss functions, multi-metric comparative evaluation against polynomial regression baselines (2D and 3D polyfit) under various operating conditions using RMSE, MAE, and depth consistency measures, validation through YOLO-based object detection for granular per-bounding-box error analysis compared to LiDAR ground truth, and detailed per-image visual analysis with depth map overlays and quantitative comparisons between the three ScaleBiasModel variants and polynomial baselines, establishing a rigorous, interpretable, and scenario-aware evaluation framework particularly relevant for real-world autonomous driving applications in commercial transport scenarios.

## Architectures of the Three ScaleBiasHead Models

The three implemented ScaleBiasHead models feature distinctive architectures designed to balance computational efficiency and predictive accuracy in depth estimation. The **Light_ScaleBiasHead** adopts a minimalist design that uses reduced channels (progressively from 4 to 32 channels) and components optimized for efficiency: it employs a LightSEBlock with a reduced reduction factor of 8, a SimplifiedMultiScale with only two branches (1x1 and 3x3 convolution) for multi-scale feature fusion, and an EfficientDepthwiseConv that separates depthwise and pointwise operations to reduce computational complexity, culminating in a simplified fully-connected head that directly maps from 32 to 3 output parameters. The **Moderate_ScaleBiasHead** implements a balanced approach with a sequential convolutional block that uses wider kernels (initially 5x5) followed by standard 3x3 convolutions, processing channels from 4 to 32 through two stages of max pooling, and ends with an AdaptiveAvgPool2d that reduces feature maps to a fixed size (4x4) before a more robust fully-connected head with 128 hidden neurons and 0.5 dropout for regularization. The **Heavy_ScaleBiasHead** represents the most sophisticated architecture, incorporating an advanced MultiScaleFeatureFusion with four parallel branches (1x1, 3x3, 5x5 convolution, and max pooling followed by 1x1 convolution) to capture features at different spatial scales, a standard SEBlock with a reduction factor of 16 for channel attention mechanism, a DepthwiseSeparableConv with separate batch normalization for depthwise and pointwise operations, and a two-stage fully-connected head (64→32→3) with dropout to maximize the model's representational capacity.

## DepthAnything2
DepthAnything2 Small is a lightweight yet powerful model for monocular depth estimation, meaning it predicts depth information from a single image. Built on the DPT architecture with a DINOv2 backbone, it processes visual input using transformer-based methods. The model is trained on a massive dataset of 595,000 synthetic labeled images and over 62 million real unlabeled images, which allows it to achieve highly accurate and robust depth predictions across diverse scenes. Compared to previous versions and Stable Diffusion-based models, DepthAnything2 Small is significantly faster (about 10 times) and more efficient, while still capturing fine-grained details in depth maps. With only 24.8 million parameters, it is optimized for speed and resource efficiency, making it suitable for real-time applications like robotics, augmented reality, and autonomous navigation, especially where computational resources are limited.

## YOLO11n
YOLO11n is the nano-sized, ultra-lightweight variant of the YOLO11 object detection model, designed for high efficiency and real-time performance on resource-constrained devices. Despite its compact architecture, YOLO11n leverages key innovations from the YOLO11 family—including enhanced backbone and neck components, advanced spatial attention mechanisms, and optimized Convolution-BatchNorm-SiLU (CBS) blocks—to achieve strong detection accuracy with minimal computational overhead. Like all YOLO11 models, YOLO11n supports a broad range of computer vision tasks such as object detection, instance segmentation, image classification, pose estimation, and oriented object detection, making it highly versatile for edge deployments and real-time applications.

## MAN TruckScenes Mini
The MAN TruckScenes Mini dataset offers rich multimodal annotations from a comprehensive sensor suite specifically designed for autonomous trucking. Each scene includes synchronized data from 4 RGB cameras, 6 lidar sensors, and 6 radar sensors, providing dense spatial coverage and robust perception in diverse conditions. The lidar sensors deliver high-resolution 3D point clouds, while the radar sensors—featuring 4D capability—capture both range-azimuth and elevation information, yielding around 2,600 points per sample with nearly 360-degree coverage. Additionally, the dataset incorporates precise vehicle state and localization data from two IMUs and a high-precision RTK-GNSS unit. All objects within a range of over 230 meters are manually annotated with 3D bounding boxes, covering 27 object classes and 15 attributes, and are tracked throughout each scene. These detailed annotations support a wide range of perception tasks, including 3D object detection, tracking, and sensor fusion research for autonomous trucking applications.

# Flow of the scrpits 

The implementation prioritizes computational efficiency through strategic memory management and device optimization. Automatic GPU detection enables accelerated processing when available, while explicit memory cleanup prevents accumulation during batch processing. The training pipeline supports variable batch sizes with memory-conscious processing, enabling operation on systems with different computational capabilities.
This approach is necessary to address issues related to limited RAM in the Colab environment.

## Dataset Foundation and Preparation
There is strategic train-test split using specific scene selections to ensure balanced representation across different operational conditions. Training scenes (0, 2, 3, 6, 8) encompass diverse environments including terminal areas, city streets, and highways under clear, overcast, and rainy conditions  . Test scenes (1, 4, 5, 7, 9) are deliberately chosen to include challenging scenarios such as snow conditions, roadworks, overpass structures, and various lighting conditions including twilight and alternative illumination  . For computational efficiency, the test set is subsampled to 30% of its original size while maintaining representational diversity.
From the dataset using TruckScenes API e some modification for every sample is extracted: the image, the mapping point-lidar_mesuraments and the description of the scene.
It's important to notice that for every img there is a limited number of lidar-points that we can use. 

## Relative Depth Estimation Through Transformer Architecture
The pipeline employs Depth-Anything V2, a state-of-the-art monocular depth estimation model that leverages transformer architecture for robust depth prediction.
During preprocessing, all input images are processed through the Depth-Anything V2 pipeline to generate relative depth heatmaps . These heatmaps capture spatial depth relationships but require transformation to absolute depth measurements for practical applications . The relative depth maps are then inverted using the mathematical operation `depth_rel = depth_rel_max - depth_rel_img` to align with conventional depth representations where larger values correspond to greater distances.

## Polynomial Regression for Affine Parameter Extraction
Two distinct polynomial regression are implementes, that approaches to establish baseline relationships between relative and absolute depth measurements . Linear regression models the relationship as $$D_{abs} = a \cdot D_{rel} + b$$, providing a simple two-parameter transformation . Quadratic regression extends this to $$D_{abs} = a \cdot D_{rel}^2 + b \cdot D_{rel} + c$$, capturing non-linear depth relationships through three parameters .Both approaches utilize NumPy's `polyfit` function to compute optimal coefficients through least-squares optimization. 
The polynomial fitting process involves extracting valid pixel coordinates from LiDAR point cloud projections onto the camera image plane, filtering out invalid depth values, and computing regression coefficients that minimize the mean squared error between predicted and measured absolute depths. These polynomial parameters serve dual purposes: establishing baseline performance metrics and providing supervision signals for neural network training.

## Neural Network Architecture and Training Strategy
The core innovation lies in the ScaleBiasHead neural network, a custom architecture designed to predict affine transformation parameters through learned feature representations  . This network processes 4-channel input tensors combining RGB imagery with relative depth information to output three affine parameters (a, b, c) for quadratic depth transformation. The architectures of 3 models are described in the previous paragraph.

Training employs a hybrid loss function combining depth prediction accuracy with parameter regularization. 
The depth loss component uses mean squared error to measure differences between transformed relative depths and LiDAR ground truth measurements  . 
The parameter loss applies Huber loss to encourage predicted affine parameters to remain close to polynomial regression baselines, providing stability and physical plausibility . 
The total loss function is formulated as:
$$L_{total} = L_{depth} + 0.5 \times L_{parameter}$$
Early stopping with patience=5 prevents overfitting by monitoring validation loss and terminating training when performance plateaus.

## Comprehensive Evaluation Methodology
The evaluation framework implements a three-tier approach progressing from abstract loss metrics to physically interpretable measurements.
Global evaluation computes test losses across the entire dataset, comparing neural network predictions against polynomial baselines using standardized loss functions. 
Condition-specific evaluation filters the dataset by lighting conditions (illuminated, dark, other_lighting) to assess performance under varying environmental scenarios.
The most innovative aspect involves object-level evaluation through YOLO integration, providing tangible metrics expressed in physical units. YOLO11n object detection generates bounding boxes for detected objects, enabling per-object depth analysis . For each bounding box, the system computes depth statistics using both model-predicted and polynomial-derived affine parameters, comparing these against LiDAR ground truth measurements within the box boundaries  .

Key evaluation metrics include mean absolute error measured in meters, per-point error normalized by LiDAR point density, and comparative analysis between different approaches. 
This progression from dataset-wide metrics to object-specific measurements provides comprehensive insight into model performance across different scales and conditions. 



